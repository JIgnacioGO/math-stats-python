{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.3.3: Eigenvectores y Eigenvalores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos de Aprendizaje\n",
    "\n",
    "Al completar este notebook, ser√°s capaz de:\n",
    "\n",
    "- **Definir** un eigenvector y un eigenvalor con la ecuaci√≥n fundamental $A\\vec{v} = \\lambda\\vec{v}$.\n",
    "- **Interpretar** geom√©tricamente a los eigenvectores como los **ejes de inercia o ejes principales de una transformaci√≥n**.\n",
    "- **Calcular** los eigenvalores y eigenvectores de una matriz usando NumPy.\n",
    "- **Comprender** el concepto de **eigendescomposici√≥n** de una matriz.\n",
    "- **Aplicar** la eigendescomposici√≥n a una matriz de covarianza para encontrar los **Componentes Principales (PCA)** de un dataset, la culminaci√≥n de nuestro viaje por el √°lgebra lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Celda de Configuraci√≥n (Oculta) ---\n",
    "%display latex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_eigen_transformation(matrix, vectors, ax=None, title='Transformaci√≥n de Vectores'):\n",
    "    standalone = ax is None\n",
    "    if standalone:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    colors = ['#0072B2', '#E69F00', '#D55E00', '#CC79A7']\n",
    "    \n",
    "    for i, v in enumerate(vectors):\n",
    "        color = colors[i % len(colors)]\n",
    "        transformed_v = matrix @ v\n",
    "        # Vector original\n",
    "        ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, \n",
    "                  color=color, label=f'v{i+1}')\n",
    "        # Vector transformado\n",
    "        ax.quiver(0, 0, transformed_v[0], transformed_v[1], angles='xy', scale_units='xy', scale=1, \n",
    "                  color=color, linestyle='--', alpha=0.7, label=f'A¬∑v{i+1}')\n",
    "    \n",
    "    limit = np.max(np.abs(np.hstack([vectors, matrix@np.array(vectors).T]))) * 1.2\n",
    "    ax.set_xlim(-limit, limit); ax.set_ylim(-limit, limit)\n",
    "    ax.set_aspect('equal'); ax.grid(True, linestyle='--')\n",
    "    ax.axhline(0, c='black', lw=0.5); ax.axvline(0, c='black', lw=0.5)\n",
    "    ax.legend(); ax.set_title(title)\n",
    "    \n",
    "    if standalone:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## ‚öôÔ∏è El Arsenal de Datasets: Nuestra Fuente de Ejercicios\n",
    "\n",
    "Para este tema culminante, usaremos datasets que tengan una estructura de covarianza interesante. El objetivo es descubrir los \"ejes\" ocultos de estos datos. Nuestro generador de matrices especiales tambi√©n ser√° crucial para crear transformaciones con eigenvectores predecibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURACI√ìN DE DATASETS ===\n",
    "from src.data_generation.create_student_performance import create_student_performance_data\n",
    "from src.data_generation.create_special_matrices import create_special_matrices\n",
    "from src.data_generation.create_edge_cases import create_edge_cases\n",
    "\n",
    "# Configuraci√≥n centralizada de aleatoriedad para REPRODUCIBILIDAD\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "# === Generaci√≥n de Datasets y Matrices para este Notebook ===\n",
    "\n",
    "# üí° CONTEXTO PEDAG√ìGICO: Hilo Conductor (Aplicaci√≥n a PCA)\n",
    "# Este es el gran final. Aplicaremos la eigendescomposici√≥n a la matriz de covarianza\n",
    "# de nuestros estudiantes para encontrar los Componentes Principales (PCA) de los datos.\n",
    "datos_estudiantes = create_student_performance_data(rng, simplified=True, n_samples=200)\n",
    "\n",
    "# üí° CONTEXTO PEDAG√ìGICO: Matrices con Ejes Claros\n",
    "# Una matriz sim√©trica es ideal para ilustrar eigenvectores, ya que sus eigenvectores\n",
    "# son siempre ortogonales y representan los ejes de una elipse de transformaci√≥n.\n",
    "matriz_simetrica = create_special_matrices(rng, matrix_type='symmetric', size=(2, 2))\n",
    "\n",
    "# üí° CONTEXTO PEDAG√ìGICO: Datos Correlacionados para PCA\n",
    "# Generaremos datos con una fuerte correlaci√≥n para que PCA pueda encontrar de forma\n",
    "# muy evidente el eje principal de la varianza.\n",
    "datos_correlacionados = create_edge_cases(rng, case_type='multicollinear', n_samples=200)[['x1', 'x2']]\n",
    "\n",
    "print(\"Datasets y matrices generados y listos para usar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. La Idea Intuitiva: \"Los Ejes de una Transformaci√≥n\"\n",
    "\n",
    "Imagina una transformaci√≥n lineal como un estiramiento o compresi√≥n del espacio. La mayor√≠a de los vectores cambiar√°n tanto su longitud como su direcci√≥n. Sin embargo, existen unos vectores \"especiales\" que son privilegiados: su direcci√≥n no cambia, solo son escalados (estirados o encogidos).\n",
    "\n",
    "- **Eigenvectores ($\\vec{v}$):** Los vectores cuya direcci√≥n no cambia bajo la transformaci√≥n $A$. Son los **ejes de acci√≥n** de la matriz.\n",
    "- **Eigenvalores ($\\lambda$):** El factor de escala (un escalar) correspondiente a cada eigenvector. Nos dice *cu√°nto* se estira o encoge el eigenvector.\n",
    "\n",
    "Esta relaci√≥n √∫nica se captura en la ecuaci√≥n m√°s famosa del √°lgebra lineal:\n",
    "$$ A\\vec{v} = \\lambda\\vec{v} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El Proceso de C√°lculo\n",
    "\n",
    "Para encontrar estos valores, reorganizamos la ecuaci√≥n a $(A - \\lambda I)\\vec{v} = \\vec{0}$. Para que exista una soluci√≥n no trivial para $\\vec{v}$ (es decir, $\\vec{v} \\neq \\vec{0}$), la matriz $(A - \\lambda I)$ debe ser singular. Esto significa que su determinante debe ser cero, lo que nos lleva a la **Ecuaci√≥n Caracter√≠stica**:\n",
    "$$ \\det(A - \\lambda I) = 0 $$\n",
    "Resolviendo esta ecuaci√≥n para $\\lambda$ obtenemos los eigenvalores. Luego, para cada $\\lambda$, resolvemos el sistema de ecuaciones para encontrar el $\\vec{v}$ correspondiente (el eigenvector), que formar√° el kernel (espacio nulo) de $(A - \\lambda I)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo Demostrativo 1: C√°lculo y Verificaci√≥n Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DATOS: Usamos nuestra matriz sim√©trica generada.\n",
    "A = matriz_simetrica\n",
    "\n",
    "# 2. APLICACI√ìN: Calculamos los eigenvalores y eigenvectores con NumPy.\n",
    "eigenvalores, eigenvectores = np.linalg.eig(A)\n",
    "lambda1, lambda2 = eigenvalores\n",
    "v1, v2 = eigenvectores[:, 0], eigenvectores[:, 1]\n",
    "\n",
    "# 3. INTERPRETACI√ìN\n",
    "print(f\"Matriz de Transformaci√≥n A:\\n{np.round(A, 2)}\")\n",
    "print(f\"\\nEigenvalor 1 (Œª1): {lambda1:.2f} con Eigenvector v1: {np.round(v1, 2)}\")\n",
    "print(f\"Eigenvalor 2 (Œª2): {lambda2:.2f} con Eigenvector v2: {np.round(v2, 2)}\")\n",
    "\n",
    "# Verificamos la ecuaci√≥n A¬∑v = Œª¬∑v para el primer par\n",
    "Av1 = A @ v1\n",
    "lambda1_v1 = lambda1 * v1\n",
    "print(f\"\\nVerificaci√≥n para v1: A¬∑v1 = {np.round(Av1, 2)}, Œª1¬∑v1 = {np.round(lambda1_v1, 2)}\")\n",
    "print(f\"¬øSon iguales? {np.allclose(Av1, lambda1_v1)}\")\n",
    "\n",
    "# 4. VISUALIZACI√ìN\n",
    "# Tambi√©n incluimos un vector que NO es un eigenvector para ver la diferencia.\n",
    "v_no_eigen = np.array([1, 0])\n",
    "plot_eigen_transformation(A, [v1, v2, v_no_eigen], title='v1 y v2 (s√≥lidos) son eigenvectores, v3 no lo es.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Aplicaci√≥n Estrella: An√°lisis de Componentes Principales (PCA)\n",
    "\n",
    "Llegamos al punto donde todos los conceptos se unen. El PCA es una t√©cnica de reducci√≥n de dimensionalidad que busca encontrar las \"direcciones de m√°xima varianza\" en un conjunto de datos. Estas direcciones son, precisamente, los **eigenvectores de la matriz de covarianza** del dataset.\n",
    "\n",
    "- **Eigenvectores (Componentes Principales):** Nos dan los nuevos ejes (ortogonales) sobre los cuales proyectar los datos.\n",
    "- **Eigenvalores:** Nos dicen cu√°nta varianza de los datos es \"explicada\" por cada uno de esos ejes. Un eigenvalor grande corresponde a un componente principal importante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo Demostrativo 2: Encontrando los Ejes Principales de Nuestros Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PREPARAR LOS DATOS: PCA requiere que los datos est√©n centrados en el origen.\n",
    "datos = datos_estudiantes[['horas_estudio', 'calificacion_examen']].values\n",
    "datos_centrados = datos - np.mean(datos, axis=0)\n",
    "\n",
    "# 2. CALCULAR LA MATRIZ DE COVARIANZA\n",
    "# Esta matriz 2x2 describe la varianza y covarianza de nuestras dos features.\n",
    "cov_matrix = np.cov(datos_centrados, rowvar=False)\n",
    "\n",
    "# 3. EIGENDESCOMPOSICI√ìN DE LA MATRIZ DE COVARIANZA\n",
    "eigenvalores, eigenvectores = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Ordenamos de mayor a menor eigenvalor\n",
    "idx = eigenvalores.argsort()[::-1]\n",
    "eigenvalores = eigenvalores[idx]\n",
    "eigenvectores = eigenvectores[:, idx]\n",
    "\n",
    "# 4. INTERPRETACI√ìN\n",
    "print(\"Matriz de Covarianza:\")\n",
    "print(np.round(cov_matrix, 2))\n",
    "print(f\"\\nEigenvalores (Varianza explicada por cada eje): {np.round(eigenvalores, 2)}\")\n",
    "print(f\"Eigenvectores (Componentes Principales):\\n{np.round(eigenvectores, 2)}\")\n",
    "\n",
    "varianza_explicada = eigenvalores / np.sum(eigenvalores)\n",
    "print(f\"\\nEl primer componente principal explica el {varianza_explicada[0]:.1%} de la varianza total.\")\n",
    "\n",
    "# 5. VISUALIZACI√ìN\n",
    "fig, ax = plt.subplots(figsize=(9, 9))\n",
    "ax.scatter(datos_centrados[:, 0], datos_centrados[:, 1], alpha=0.5, label='Datos de Estudiantes (Centrados)')\n",
    "\n",
    "# Dibujamos los eigenvectores escalados por sus eigenvalores\n",
    "for i in range(eigenvectores.shape[1]):\n",
    "    vec = eigenvectores[:, i]\n",
    "    # Escalar para visualizaci√≥n: 3 * sqrt(eigenvalor) para que se vea como 3 desviaciones est√°ndar\n",
    "    scaled_vec = vec * 3 * np.sqrt(eigenvalores[i]) \n",
    "    ax.quiver(0, 0, scaled_vec[0], scaled_vec[1], angles='xy', scale_units='xy', scale=1, \n",
    "              color=['#D55E00', '#009E73'][i], width=0.01, label=f'Componente Principal {i+1}')\n",
    "\n",
    "ax.set_aspect('equal'); ax.grid(True, linestyle='--'); ax.legend()\n",
    "ax.set_title(\"PCA: Eigenvectores de la Matriz de Covarianza\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Ejercicios Guiados con Scaffolding (8+)\n",
    "Rellena las partes marcadas con `# COMPLETAR` para afianzar tu comprensi√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 1: Verificar un Eigenvector ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS\n",
    "A = np.array([[5, -1], [3, 1]])\n",
    "v_candidato = np.array([1, 1])\n",
    "lambda_candidato = 4\n",
    "\n",
    "# TODO 1: Calcula el lado izquierdo de la ecuaci√≥n: A @ v\n",
    "lado_izquierdo = # COMPLETAR\n",
    "\n",
    "# TODO 2: Calcula el lado derecho de la ecuaci√≥n: Œª * v\n",
    "lado_derecho = # COMPLETAR\n",
    "\n",
    "# TODO 3: Compara si ambos lados son (aproximadamente) iguales.\n",
    "es_eigenvector = # COMPLETAR\n",
    "\n",
    "# VERIFICACI√ìN\n",
    "assert es_eigenvector, \"El vector candidato deber√≠a ser un eigenvector con el eigenvalor dado.\"\n",
    "print(f\"‚úÖ ¬°Correcto! A¬∑v = {lado_izquierdo} y Œª¬∑v = {lado_derecho}, por lo que v es un eigenvector de A.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 2: Calcular Eigenvalores y Eigenvectores ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS\n",
    "A = np.array([[2, 7], [7, 2]])\n",
    "\n",
    "# TODO 1: Usa np.linalg.eig() para obtener los eigenvalores y eigenvectores de A.\n",
    "eigenvalores, eigenvectores = # COMPLETAR\n",
    "\n",
    "# VERIFICACI√ìN\n",
    "assert eigenvalores.shape == (2,), \"Deber√≠a haber 2 eigenvalores.\"\n",
    "assert eigenvectores.shape == (2, 2), \"Deber√≠a haber 2 eigenvectores de 2 dimensiones.\"\n",
    "# Verificamos la ecuaci√≥n para el primer par\n",
    "assert np.allclose(A @ eigenvectores[:, 0], eigenvalores[0] * eigenvectores[:, 0])\n",
    "print(\"‚úÖ ¬°C√°lculo correcto!\")\n",
    "print(f\"Eigenvalores: {eigenvalores}\")\n",
    "print(f\"Eigenvectores (como columnas):\\n{eigenvectores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 3: Eigenvalores de una Matriz de Proyecci√≥n ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS: Una matriz que proyecta vectores sobre el eje X.\n",
    "P = np.array([[1, 0], [0, 0]])\n",
    "\n",
    "# TODO 1: Piensa geom√©tricamente. ¬øQu√© vectores no cambian de direcci√≥n al ser proyectados sobre el eje X?\n",
    "# ¬øY qu√© vectores son 'aplastados' a cero? Esto te dar√° los eigenvectores y eigenvalores.\n",
    "eigenvalor1_esperado = # COMPLETAR (Para vectores ya en el eje X)\n",
    "eigenvalor2_esperado = # COMPLETAR (Para vectores en el eje Y)\n",
    "\n",
    "# TODO 2: Calcula los eigenvalores num√©ricamente para confirmar.\n",
    "eigenvalores, _ = # COMPLETAR\n",
    "\n",
    "# VERIFICACI√ìN\n",
    "assert eigenvalor1_esperado == 1\n",
    "assert eigenvalor2_esperado == 0\n",
    "assert sorted(eigenvalores.tolist()) == [0, 1]\n",
    "print(\"‚úÖ ¬°An√°lisis correcto!\")\n",
    "print(\"Un vector en el eje X se queda igual (Œª=1), un vector en el eje Y es aplastado a cero (Œª=0).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 4: Eigendescomposici√≥n (A = P¬∑D¬∑P‚Åª¬π) ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS: La misma matriz sim√©trica A del ejercicio guiado 2.\n",
    "A = np.array([[2, 7], [7, 2]])\n",
    "\n",
    "# TODO 1: Obt√©n los eigenvalores (D) y eigenvectores (P).\n",
    "lambdas, P = # COMPLETAR\n",
    "\n",
    "# TODO 2: Construye la matriz diagonal D.\n",
    "# PISTA: Usa np.diag().\n",
    "D = # COMPLETAR\n",
    "\n",
    "# TODO 3: Calcula la inversa de P.\n",
    "# PISTA: Usa np.linalg.inv().\n",
    "P_inv = # COMPLETAR\n",
    "\n",
    "# TODO 4: Reconstruye la matriz original multiplicando P @ D @ P_inv.\n",
    "A_reconstruida = # COMPLETAR\n",
    "\n",
    "# VERIFICACI√ìN\n",
    "assert np.allclose(A, A_reconstruida)\n",
    "print(\"‚úÖ ¬°Eigendescomposici√≥n verificada!\")\n",
    "print(f\"La matriz original A puede ser reconstruida a partir de sus eigenvectores y eigenvalores.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# 5. Banco de Ejercicios Pr√°cticos (30+)\n",
    "Ahora te toca a ti. Resuelve estos ejercicios para consolidar tu conocimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte A: C√°lculo y Verificaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A1 (üü¢ F√°cil):** Encuentra los eigenvalores y eigenvectores de $A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A2 (üü¢ F√°cil):** Encuentra los eigenvalores y eigenvectores de $B = \\begin{pmatrix} 5 & 0 \\\\ 0 & -3 \\end{pmatrix}$. ¬øQu√© observas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A3 (üü° Medio):** Verifica que $\\vec{v}=[1,1]$ es un eigenvector de $M = \\begin{pmatrix} 4 & 1 \\\\ 2 & 3 \\end{pmatrix}$. ¬øCu√°l es su eigenvalor correspondiente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A4 (üü° Medio):** Genera una matriz sim√©trica 3x3. Calcula sus eigenvalores y eigenvectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A5 (üî¥ Reto):** La traza de una matriz (la suma de su diagonal) es igual a la suma de sus eigenvalores. El determinante es igual al producto de sus eigenvalores. Genera una matriz 3x3 aleatoria y verifica estas dos propiedades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte B: Interpretaci√≥n Geom√©trica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B1 (üü¢ F√°cil):** Una matriz de reflexi√≥n sobre el eje y es $F = \\begin{pmatrix} -1 & 0 \\\\ 0 & 1 \\end{pmatrix}$. ¬øCu√°les son sus eigenvectores y eigenvalores? Pi√©nsalo geom√©tricamente antes de calcular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B2 (üü¢ F√°cil):** ¬øCu√°les son los eigenvalores de la matriz identidad 2x2? ¬øY sus eigenvectores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B3 (üü° Medio):** Una matriz de cizalla (shear) es $S = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}$. Calcula sus eigenvectores y eigenvalores. ¬øPor qu√© solo hay un eje de eigenvectores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B4 (üî¥ Reto):** Una matriz de rotaci√≥n de 45 grados en 2D no tiene eigenvectores reales (ning√∫n vector real mantiene su direcci√≥n). Calcula sus eigenvalores y observa que son n√∫meros complejos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte C: Aplicaci√≥n a PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C1 (üü° Medio):** Toma el dataset `datos_correlacionados` que generamos. C√©ntralo, calcula su matriz de covarianza y encuentra sus eigenvectores y eigenvalores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C2 (üü° Medio):** Para el ejercicio C1, ¬øqu√© porcentaje de la varianza total es explicada por el primer componente principal (el eigenvector con el mayor eigenvalor)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C3 (üî¥ Reto):** En el Ejemplo 2 (PCA), el primer componente principal es el primer eigenvector (`pc1 = eigenvectores[:, 0]`). Proyecta todos los `datos_centrados` sobre la l√≠nea definida por este vector. La f√≥rmula para las proyecciones es `proyecciones = datos_centrados @ pc1`. El resultado ser√° un array 1D. Grafica un histograma de estas proyecciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Mini-Quiz de Autoevaluaci√≥n\n",
    "\n",
    "*Responde estas preguntas para verificar tu comprensi√≥n.*\n",
    "\n",
    "1. ¬øQu√© ecuaci√≥n se debe resolver para encontrar los eigenvalores de una matriz A?\n",
    "2. En el contexto de PCA, ¬øqu√© representan los eigenvectores de la matriz de covarianza?\n",
    "3. Si un eigenvalor es 0, ¬øqu√© puedes concluir sobre el determinante de la matriz y su invertibilidad?\n",
    "4. Verdadero o Falso: Una matriz 3x3 siempre tiene 3 eigenvectores linealmente independientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Pr√≥ximos Pasos (Cierre del √Årea 1.1)\n",
    "\n",
    "**¬°Felicidades, has completado los fundamentos del √Ålgebra Lineal!** Has construido un entendimiento s√≥lido desde la definici√≥n de un vector hasta la aplicaci√≥n de eigenvectores en una de las t√©cnicas m√°s importantes de la ciencia de datos: PCA. Ahora tienes la base conceptual y computacional para entender c√≥mo se manipulan los datos en alta dimensi√≥n.\n",
    "\n",
    "- En el siguiente gran bloque, **√Årea 1.2: C√°lculo**, cambiaremos de marcha para explorar la matem√°tica del cambio continuo. Descubriremos c√≥mo las derivadas nos permiten encontrar la \"pendiente\" en espacios de alta dimensi√≥n, una idea fundamental para entrenar y optimizar casi todos los modelos de Machine Learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 10.x",
   "language": "sagemath",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
