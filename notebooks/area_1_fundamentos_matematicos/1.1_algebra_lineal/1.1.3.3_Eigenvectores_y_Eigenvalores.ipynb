{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.3.3: Eigenvectores y Eigenvalores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos de Aprendizaje\n",
    "\n",
    "Al completar este notebook, serás capaz de:\n",
    "\n",
    "- **Definir** un eigenvector y un eigenvalor con la ecuación fundamental $A\\vec{v} = \\lambda\\vec{v}$.\n",
    "- **Interpretar** geométricamente a los eigenvectores como los **ejes de inercia o ejes principales de una transformación**.\n",
    "- **Calcular** los eigenvalores y eigenvectores de una matriz usando NumPy.\n",
    "- **Comprender** el concepto de **eigendescomposición** de una matriz.\n",
    "- **Aplicar** la eigendescomposición a una matriz de covarianza para encontrar los **Componentes Principales (PCA)** de un dataset, la culminación de nuestro viaje por el álgebra lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Celda de Configuración (Oculta) ---\n",
    "%display latex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_eigen_transformation(matrix, vectors, ax=None, title='Transformación de Vectores'):\n",
    "    standalone = ax is None\n",
    "    if standalone:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    colors = ['#0072B2', '#E69F00', '#D55E00', '#CC79A7']\n",
    "    \n",
    "    for i, v in enumerate(vectors):\n",
    "        color = colors[i % len(colors)]\n",
    "        transformed_v = matrix @ v\n",
    "        # Vector original\n",
    "        ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, \n",
    "                  color=color, label=f'v{i+1}')\n",
    "        # Vector transformado\n",
    "        ax.quiver(0, 0, transformed_v[0], transformed_v[1], angles='xy', scale_units='xy', scale=1, \n",
    "                  color=color, linestyle='--', alpha=0.7, label=f'A·v{i+1}')\n",
    "    \n",
    "    limit = np.max(np.abs(np.hstack([vectors, matrix@np.array(vectors).T]))) * 1.2\n",
    "    ax.set_xlim(-limit, limit); ax.set_ylim(-limit, limit)\n",
    "    ax.set_aspect('equal'); ax.grid(True, linestyle='--')\n",
    "    ax.axhline(0, c='black', lw=0.5); ax.axvline(0, c='black', lw=0.5)\n",
    "    ax.legend(); ax.set_title(title)\n",
    "    \n",
    "    if standalone:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## ⚙️ El Arsenal de Datasets: Nuestra Fuente de Ejercicios\n",
    "\n",
    "Para este tema culminante, usaremos datasets que tengan una estructura de covarianza interesante. El objetivo es descubrir los \"ejes\" ocultos de estos datos. Nuestro generador de matrices especiales también será crucial para crear transformaciones con eigenvectores predecibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURACIÓN DE DATASETS ===\n",
    "from src.data_generation.create_student_performance import create_student_performance_data\n",
    "from src.data_generation.create_special_matrices import create_special_matrices\n",
    "from src.data_generation.create_edge_cases import create_edge_cases\n",
    "\n",
    "# Configuración centralizada de aleatoriedad para REPRODUCIBILIDAD\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "# === Generación de Datasets y Matrices para este Notebook ===\n",
    "\n",
    "# 💡 CONTEXTO PEDAGÓGICO: Hilo Conductor (Aplicación a PCA)\n",
    "# Este es el gran final. Aplicaremos la eigendescomposición a la matriz de covarianza\n",
    "# de nuestros estudiantes para encontrar los Componentes Principales (PCA) de los datos.\n",
    "datos_estudiantes = create_student_performance_data(rng, simplified=True, n_samples=200)\n",
    "\n",
    "# 💡 CONTEXTO PEDAGÓGICO: Matrices con Ejes Claros\n",
    "# Una matriz simétrica es ideal para ilustrar eigenvectores, ya que sus eigenvectores\n",
    "# son siempre ortogonales y representan los ejes de una elipse de transformación.\n",
    "matriz_simetrica = create_special_matrices(rng, matrix_type='symmetric', size=(2, 2))\n",
    "\n",
    "# 💡 CONTEXTO PEDAGÓGICO: Datos Correlacionados para PCA\n",
    "# Generaremos datos con una fuerte correlación para que PCA pueda encontrar de forma\n",
    "# muy evidente el eje principal de la varianza.\n",
    "datos_correlacionados = create_edge_cases(rng, case_type='multicollinear', n_samples=200)[['x1', 'x2']]\n",
    "\n",
    "print(\"Datasets y matrices generados y listos para usar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. La Idea Intuitiva: \"Los Ejes de una Transformación\"\n",
    "\n",
    "Imagina una transformación lineal como un estiramiento o compresión del espacio. La mayoría de los vectores cambiarán tanto su longitud como su dirección. Sin embargo, existen unos vectores \"especiales\" que son privilegiados: su dirección no cambia, solo son escalados (estirados o encogidos).\n",
    "\n",
    "- **Eigenvectores ($\\vec{v}$):** Los vectores cuya dirección no cambia bajo la transformación $A$. Son los **ejes de acción** de la matriz.\n",
    "- **Eigenvalores ($\\lambda$):** El factor de escala (un escalar) correspondiente a cada eigenvector. Nos dice *cuánto* se estira o encoge el eigenvector.\n",
    "\n",
    "Esta relación única se captura en la ecuación más famosa del álgebra lineal:\n",
    "$$ A\\vec{v} = \\lambda\\vec{v} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El Proceso de Cálculo\n",
    "\n",
    "Para encontrar estos valores, reorganizamos la ecuación a $(A - \\lambda I)\\vec{v} = \\vec{0}$. Para que exista una solución no trivial para $\\vec{v}$ (es decir, $\\vec{v} \\neq \\vec{0}$), la matriz $(A - \\lambda I)$ debe ser singular. Esto significa que su determinante debe ser cero, lo que nos lleva a la **Ecuación Característica**:\n",
    "$$ \\det(A - \\lambda I) = 0 $$\n",
    "Resolviendo esta ecuación para $\\lambda$ obtenemos los eigenvalores. Luego, para cada $\\lambda$, resolvemos el sistema de ecuaciones para encontrar el $\\vec{v}$ correspondiente (el eigenvector), que formará el kernel (espacio nulo) de $(A - \\lambda I)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo Demostrativo 1: Cálculo y Verificación Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DATOS: Usamos nuestra matriz simétrica generada.\n",
    "A = matriz_simetrica\n",
    "\n",
    "# 2. APLICACIÓN: Calculamos los eigenvalores y eigenvectores con NumPy.\n",
    "eigenvalores, eigenvectores = np.linalg.eig(A)\n",
    "lambda1, lambda2 = eigenvalores\n",
    "v1, v2 = eigenvectores[:, 0], eigenvectores[:, 1]\n",
    "\n",
    "# 3. INTERPRETACIÓN\n",
    "print(f\"Matriz de Transformación A:\\n{np.round(A, 2)}\")\n",
    "print(f\"\\nEigenvalor 1 (λ1): {lambda1:.2f} con Eigenvector v1: {np.round(v1, 2)}\")\n",
    "print(f\"Eigenvalor 2 (λ2): {lambda2:.2f} con Eigenvector v2: {np.round(v2, 2)}\")\n",
    "\n",
    "# Verificamos la ecuación A·v = λ·v para el primer par\n",
    "Av1 = A @ v1\n",
    "lambda1_v1 = lambda1 * v1\n",
    "print(f\"\\nVerificación para v1: A·v1 = {np.round(Av1, 2)}, λ1·v1 = {np.round(lambda1_v1, 2)}\")\n",
    "print(f\"¿Son iguales? {np.allclose(Av1, lambda1_v1)}\")\n",
    "\n",
    "# 4. VISUALIZACIÓN\n",
    "# También incluimos un vector que NO es un eigenvector para ver la diferencia.\n",
    "v_no_eigen = np.array([1, 0])\n",
    "plot_eigen_transformation(A, [v1, v2, v_no_eigen], title='v1 y v2 (sólidos) son eigenvectores, v3 no lo es.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Aplicación Estrella: Análisis de Componentes Principales (PCA)\n",
    "\n",
    "Llegamos al punto donde todos los conceptos se unen. El PCA es una técnica de reducción de dimensionalidad que busca encontrar las \"direcciones de máxima varianza\" en un conjunto de datos. Estas direcciones son, precisamente, los **eigenvectores de la matriz de covarianza** del dataset.\n",
    "\n",
    "- **Eigenvectores (Componentes Principales):** Nos dan los nuevos ejes (ortogonales) sobre los cuales proyectar los datos.\n",
    "- **Eigenvalores:** Nos dicen cuánta varianza de los datos es \"explicada\" por cada uno de esos ejes. Un eigenvalor grande corresponde a un componente principal importante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo Demostrativo 2: Encontrando los Ejes Principales de Nuestros Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PREPARAR LOS DATOS: PCA requiere que los datos estén centrados en el origen.\n",
    "datos = datos_estudiantes[['horas_estudio', 'calificacion_examen']].values\n",
    "datos_centrados = datos - np.mean(datos, axis=0)\n",
    "\n",
    "# 2. CALCULAR LA MATRIZ DE COVARIANZA\n",
    "# Esta matriz 2x2 describe la varianza y covarianza de nuestras dos features.\n",
    "cov_matrix = np.cov(datos_centrados, rowvar=False)\n",
    "\n",
    "# 3. EIGENDESCOMPOSICIÓN DE LA MATRIZ DE COVARIANZA\n",
    "eigenvalores, eigenvectores = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Ordenamos de mayor a menor eigenvalor\n",
    "idx = eigenvalores.argsort()[::-1]\n",
    "eigenvalores = eigenvalores[idx]\n",
    "eigenvectores = eigenvectores[:, idx]\n",
    "\n",
    "# 4. INTERPRETACIÓN\n",
    "print(\"Matriz de Covarianza:\")\n",
    "print(np.round(cov_matrix, 2))\n",
    "print(f\"\\nEigenvalores (Varianza explicada por cada eje): {np.round(eigenvalores, 2)}\")\n",
    "print(f\"Eigenvectores (Componentes Principales):\\n{np.round(eigenvectores, 2)}\")\n",
    "\n",
    "varianza_explicada = eigenvalores / np.sum(eigenvalores)\n",
    "print(f\"\\nEl primer componente principal explica el {varianza_explicada[0]:.1%} de la varianza total.\")\n",
    "\n",
    "# 5. VISUALIZACIÓN\n",
    "fig, ax = plt.subplots(figsize=(9, 9))\n",
    "ax.scatter(datos_centrados[:, 0], datos_centrados[:, 1], alpha=0.5, label='Datos de Estudiantes (Centrados)')\n",
    "\n",
    "# Dibujamos los eigenvectores escalados por sus eigenvalores\n",
    "for i in range(eigenvectores.shape[1]):\n",
    "    vec = eigenvectores[:, i]\n",
    "    # Escalar para visualización: 3 * sqrt(eigenvalor) para que se vea como 3 desviaciones estándar\n",
    "    scaled_vec = vec * 3 * np.sqrt(eigenvalores[i]) \n",
    "    ax.quiver(0, 0, scaled_vec[0], scaled_vec[1], angles='xy', scale_units='xy', scale=1, \n",
    "              color=['#D55E00', '#009E73'][i], width=0.01, label=f'Componente Principal {i+1}')\n",
    "\n",
    "ax.set_aspect('equal'); ax.grid(True, linestyle='--'); ax.legend()\n",
    "ax.set_title(\"PCA: Eigenvectores de la Matriz de Covarianza\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Ejercicios Guiados con Scaffolding (8+)\n",
    "Rellena las partes marcadas con `# COMPLETAR` para afianzar tu comprensión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 1: Verificar un Eigenvector ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS\n",
    "A = np.array([[5, -1], [3, 1]])\n",
    "v_candidato = np.array([1, 1])\n",
    "lambda_candidato = 4\n",
    "\n",
    "# TODO 1: Calcula el lado izquierdo de la ecuación: A @ v\n",
    "lado_izquierdo = # COMPLETAR\n",
    "\n",
    "# TODO 2: Calcula el lado derecho de la ecuación: λ * v\n",
    "lado_derecho = # COMPLETAR\n",
    "\n",
    "# TODO 3: Compara si ambos lados son (aproximadamente) iguales.\n",
    "es_eigenvector = # COMPLETAR\n",
    "\n",
    "# VERIFICACIÓN\n",
    "assert es_eigenvector, \"El vector candidato debería ser un eigenvector con el eigenvalor dado.\"\n",
    "print(f\"✅ ¡Correcto! A·v = {lado_izquierdo} y λ·v = {lado_derecho}, por lo que v es un eigenvector de A.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 2: Calcular Eigenvalores y Eigenvectores ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS\n",
    "A = np.array([[2, 7], [7, 2]])\n",
    "\n",
    "# TODO 1: Usa np.linalg.eig() para obtener los eigenvalores y eigenvectores de A.\n",
    "eigenvalores, eigenvectores = # COMPLETAR\n",
    "\n",
    "# VERIFICACIÓN\n",
    "assert eigenvalores.shape == (2,), \"Debería haber 2 eigenvalores.\"\n",
    "assert eigenvectores.shape == (2, 2), \"Debería haber 2 eigenvectores de 2 dimensiones.\"\n",
    "# Verificamos la ecuación para el primer par\n",
    "assert np.allclose(A @ eigenvectores[:, 0], eigenvalores[0] * eigenvectores[:, 0])\n",
    "print(\"✅ ¡Cálculo correcto!\")\n",
    "print(f\"Eigenvalores: {eigenvalores}\")\n",
    "print(f\"Eigenvectores (como columnas):\\n{eigenvectores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 3: Eigenvalores de una Matriz de Proyección ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS: Una matriz que proyecta vectores sobre el eje X.\n",
    "P = np.array([[1, 0], [0, 0]])\n",
    "\n",
    "# TODO 1: Piensa geométricamente. ¿Qué vectores no cambian de dirección al ser proyectados sobre el eje X?\n",
    "# ¿Y qué vectores son 'aplastados' a cero? Esto te dará los eigenvectores y eigenvalores.\n",
    "eigenvalor1_esperado = # COMPLETAR (Para vectores ya en el eje X)\n",
    "eigenvalor2_esperado = # COMPLETAR (Para vectores en el eje Y)\n",
    "\n",
    "# TODO 2: Calcula los eigenvalores numéricamente para confirmar.\n",
    "eigenvalores, _ = # COMPLETAR\n",
    "\n",
    "# VERIFICACIÓN\n",
    "assert eigenvalor1_esperado == 1\n",
    "assert eigenvalor2_esperado == 0\n",
    "assert sorted(eigenvalores.tolist()) == [0, 1]\n",
    "print(\"✅ ¡Análisis correcto!\")\n",
    "print(\"Un vector en el eje X se queda igual (λ=1), un vector en el eje Y es aplastado a cero (λ=0).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 4: Eigendescomposición (A = P·D·P⁻¹) ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS: La misma matriz simétrica A del ejercicio guiado 2.\n",
    "A = np.array([[2, 7], [7, 2]])\n",
    "\n",
    "# TODO 1: Obtén los eigenvalores (D) y eigenvectores (P).\n",
    "lambdas, P = # COMPLETAR\n",
    "\n",
    "# TODO 2: Construye la matriz diagonal D.\n",
    "# PISTA: Usa np.diag().\n",
    "D = # COMPLETAR\n",
    "\n",
    "# TODO 3: Calcula la inversa de P.\n",
    "# PISTA: Usa np.linalg.inv().\n",
    "P_inv = # COMPLETAR\n",
    "\n",
    "# TODO 4: Reconstruye la matriz original multiplicando P @ D @ P_inv.\n",
    "A_reconstruida = # COMPLETAR\n",
    "\n",
    "# VERIFICACIÓN\n",
    "assert np.allclose(A, A_reconstruida)\n",
    "print(\"✅ ¡Eigendescomposición verificada!\")\n",
    "print(f\"La matriz original A puede ser reconstruida a partir de sus eigenvectores y eigenvalores.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# 5. Banco de Ejercicios Prácticos (30+)\n",
    "Ahora te toca a ti. Resuelve estos ejercicios para consolidar tu conocimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte A: Cálculo y Verificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A1 (🟢 Fácil):** Encuentra los eigenvalores y eigenvectores de $A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A2 (🟢 Fácil):** Encuentra los eigenvalores y eigenvectores de $B = \\begin{pmatrix} 5 & 0 \\\\ 0 & -3 \\end{pmatrix}$. ¿Qué observas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A3 (🟡 Medio):** Verifica que $\\vec{v}=[1,1]$ es un eigenvector de $M = \\begin{pmatrix} 4 & 1 \\\\ 2 & 3 \\end{pmatrix}$. ¿Cuál es su eigenvalor correspondiente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A4 (🟡 Medio):** Genera una matriz simétrica 3x3. Calcula sus eigenvalores y eigenvectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A5 (🔴 Reto):** La traza de una matriz (la suma de su diagonal) es igual a la suma de sus eigenvalores. El determinante es igual al producto de sus eigenvalores. Genera una matriz 3x3 aleatoria y verifica estas dos propiedades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte B: Interpretación Geométrica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B1 (🟢 Fácil):** Una matriz de reflexión sobre el eje y es $F = \\begin{pmatrix} -1 & 0 \\\\ 0 & 1 \\end{pmatrix}$. ¿Cuáles son sus eigenvectores y eigenvalores? Piénsalo geométricamente antes de calcular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B2 (🟢 Fácil):** ¿Cuáles son los eigenvalores de la matriz identidad 2x2? ¿Y sus eigenvectores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B3 (🟡 Medio):** Una matriz de cizalla (shear) es $S = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}$. Calcula sus eigenvectores y eigenvalores. ¿Por qué solo hay un eje de eigenvectores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B4 (🔴 Reto):** Una matriz de rotación de 45 grados en 2D no tiene eigenvectores reales (ningún vector real mantiene su dirección). Calcula sus eigenvalores y observa que son números complejos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte C: Aplicación a PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C1 (🟡 Medio):** Toma el dataset `datos_correlacionados` que generamos. Céntralo, calcula su matriz de covarianza y encuentra sus eigenvectores y eigenvalores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C2 (🟡 Medio):** Para el ejercicio C1, ¿qué porcentaje de la varianza total es explicada por el primer componente principal (el eigenvector con el mayor eigenvalor)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C3 (🔴 Reto):** En el Ejemplo 2 (PCA), el primer componente principal es el primer eigenvector (`pc1 = eigenvectores[:, 0]`). Proyecta todos los `datos_centrados` sobre la línea definida por este vector. La fórmula para las proyecciones es `proyecciones = datos_centrados @ pc1`. El resultado será un array 1D. Grafica un histograma de estas proyecciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Mini-Quiz de Autoevaluación\n",
    "\n",
    "*Responde estas preguntas para verificar tu comprensión.*\n",
    "\n",
    "1. ¿Qué ecuación se debe resolver para encontrar los eigenvalores de una matriz A?\n",
    "2. En el contexto de PCA, ¿qué representan los eigenvectores de la matriz de covarianza?\n",
    "3. Si un eigenvalor es 0, ¿qué puedes concluir sobre el determinante de la matriz y su invertibilidad?\n",
    "4. Verdadero o Falso: Una matriz 3x3 siempre tiene 3 eigenvectores linealmente independientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Próximos Pasos (Cierre del Área 1.1)\n",
    "\n",
    "**¡Felicidades, has completado los fundamentos del Álgebra Lineal!** Has construido un entendimiento sólido desde la definición de un vector hasta la aplicación de eigenvectores en una de las técnicas más importantes de la ciencia de datos: PCA. Ahora tienes la base conceptual y computacional para entender cómo se manipulan los datos en alta dimensión.\n",
    "\n",
    "- En el siguiente gran bloque, **Área 1.2: Cálculo**, cambiaremos de marcha para explorar la matemática del cambio continuo. Descubriremos cómo las derivadas nos permiten encontrar la \"pendiente\" en espacios de alta dimensión, una idea fundamental para entrenar y optimizar casi todos los modelos de Machine Learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 10.x",
   "language": "sagemath",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
