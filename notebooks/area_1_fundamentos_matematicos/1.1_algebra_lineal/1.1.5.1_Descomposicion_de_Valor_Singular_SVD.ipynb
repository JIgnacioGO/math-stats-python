{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.5.1: Descomposición de Valor Singular (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos de Aprendizaje\n",
    "\n",
    "Al completar este notebook, serás capaz de:\n",
    "\n",
    "- **Explicar** la SVD como la descomposición de **cualquier** matriz $A$ en $U \\Sigma V^T$.\n",
    "- **Interpretar** geométricamente cada componente: Rotación ($V^T$), Escalamiento ($\\Sigma$), y Rotación ($U$).\n",
    "- **Utilizar** la SVD para construir **aproximaciones de rango reducido** para la compresión de datos.\n",
    "- **Resolver** problemas de mínimos cuadrados de forma robusta usando la **Pseudo-inversa**, derivada de la SVD.\n",
    "- **Conectar** explícitamente la SVD con el **Análisis de Componentes Principales (PCA)**, entendiendo por qué es el método numéricamente preferido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Celda de Configuración (Oculta) ---\n",
    "%display latex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import datasets # Para la imagen de ejemplo\n",
    "\n",
    "def plot_svd_transformation(matrix):\n",
    "    t = np.linspace(0, 2*np.pi, 100)\n",
    "    circle = np.vstack((np.cos(t), np.sin(t)))\n",
    "    U, s, Vt = np.linalg.svd(matrix)\n",
    "    S = np.diag(s)\n",
    "    # Asegurarse que las matrices tengan las dimensiones correctas para 2D\n",
    "    if U.shape[0] > 2:\n",
    "        U = U[:, :2]\n",
    "    if Vt.shape[0] > 2:\n",
    "        Vt = Vt[:2, :]\n",
    "    if S.shape[0] > 2:\n",
    "        S = S[:2, :2]\n",
    "        \n",
    "    step1 = Vt @ circle\n",
    "    step2 = S @ step1\n",
    "    step3 = U @ step2\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 4, figsize=(22, 5.5))\n",
    "    titles = ['1. Original', '2. Rotación por $V^T$', '3. Escalamiento por $\\Sigma$', '4. Rotación por U (Final)']\n",
    "    data = [circle, step1, step2, step3]\n",
    "    \n",
    "    for i, ax in enumerate(axs):\n",
    "        ax.plot(data[i][0, :], data[i][1, :], lw=3)\n",
    "        ax.set_title(titles[i], fontsize=14)\n",
    "        ax.axis('equal'); ax.grid(True)\n",
    "        ax.axhline(0, color='black', lw=0.5); ax.axvline(0, color='black', lw=0.5)\n",
    "        \n",
    "    plt.suptitle('Descomposición Geométrica de una Transformación vía SVD', fontsize=18, y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## ⚙️ El Arsenal de Datasets: Nuestra Fuente de Ejercicios\n",
    "\n",
    "La SVD es la \"navaja suiza\" del álgebra lineal. Para explorarla, necesitamos datasets que nos permitan ver sus superpoderes: compresión, solución de sistemas y su conexión con PCA. Usaremos imágenes, datos de estudiantes y matrices con propiedades especiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURACIÓN DE DATASETS ===\n",
    "from src.data_generation.create_student_performance import create_student_performance_data\n",
    "from src.data_generation.create_edge_cases import create_edge_cases\n",
    "from src.data_generation.create_special_matrices import create_special_matrices\n",
    "\n",
    "# Configuración centralizada de aleatoriedad para REPRODUCIBILIDAD\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "# === Generación de Datasets y Matrices para este Notebook ===\n",
    "\n",
    "# 💡 CONTEXTO PEDAGÓGICO: Hilo Conductor (Aplicación a PCA)\n",
    "# La conexión entre SVD y PCA es uno de los resultados más importantes. Usaremos\n",
    "# nuestros datos de estudiantes para demostrar que la SVD de la matriz de datos nos da\n",
    "# directamente los componentes principales, de una forma más estable que la eigendescomposición.\n",
    "datos_estudiantes = create_student_performance_data(rng, simplified=True, n_samples=200)\n",
    "\n",
    "# 💡 CONTEXTO PEDAGÓGICO: Compresión de Datos\n",
    "# Una imagen es solo una matriz de valores de píxeles. Esto la convierte en el ejemplo\n",
    "# perfecto para visualizar la aproximación de rango reducido, la base de la compresión.\n",
    "imagen_mapache = datasets.face(gray=True)\n",
    "\n",
    "# 💡 CONTEXTO PEDAGÓGICO: Mínimos Cuadrados Robusto\n",
    "# Usaremos un dataset con multicolinealidad para demostrar cómo la pseudo-inversa,\n",
    "# calculada con SVD, puede resolver problemas de mínimos cuadrados donde las Ecuaciones Normales fallan.\n",
    "datos_multicolineales = create_edge_cases(rng, case_type='multicollinear', n_samples=50)\n",
    "\n",
    "print(\"Datasets y matrices generados y listos para usar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. La Gran Intuición: Toda Matriz es Rotación, Escalamiento y Rotación\n",
    "\n",
    "La Descomposición de Valor Singular (SVD) es posiblemente el teorema más importante del álgebra lineal aplicada. Afirma que **cualquier** matriz $A$ de $m \\times n$ (incluso no cuadrada) puede ser factorizada en el producto de tres matrices con interpretaciones geométricas muy claras:\n",
    "$$ A = U \\Sigma V^T $$\n",
    "\n",
    "- **$V^T$ (Primera Rotación):** Una matriz **ortogonal** ($n \\times n$) que rota el espacio de entrada sin cambiar longitudes ni ángulos. Alinea los ejes del espacio de entrada con los ejes principales de la transformación.\n",
    "- **$\\Sigma$ (Escalamiento):** Una matriz **diagonal** ($m \\times n$) que estira o encoge el espacio a lo largo de los ejes rotados. Sus elementos diagonales $\\sigma_i$ son los **valores singulares** de A, y están ordenados de mayor a menor ($\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0$).\n",
    "- **$U$ (Segunda Rotación):** Otra matriz **ortogonal** ($m \\times m$) que rota el espacio de salida resultante a su orientación final.\n",
    "\n",
    "La SVD nos da un desglose completo de la \"receta\" de cualquier transformación lineal: una rotación, un escalamiento puro a lo largo de ejes perpendiculares, y una rotación final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo Demostrativo 1: Visualización Geométrica de la SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DATOS: Una matriz de transformación 2x2 simple.\n",
    "A = np.array([[3, 0], [2, 2]])\n",
    "\n",
    "# 2. APLICACIÓN Y VISUALIZACIÓN\n",
    "# La función de ayuda `plot_svd_transformation` calcula la SVD y aplica cada paso\n",
    "# a un círculo unitario para que podamos ver la descomposición de la acción.\n",
    "plot_svd_transformation(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Aplicación 1: Aproximación de Rango Reducido (Compresión)\n",
    "\n",
    "Los valores singulares en $\\Sigma$ están ordenados por \"importancia\". El primer valor singular, $\\sigma_1$, captura la dirección de mayor acción de la matriz. Al quedarnos solo con los `k` valores singulares más grandes, podemos reconstruir una matriz $A_k = U_k \\Sigma_k V_k^T$ que es la **mejor aproximación posible de rango `k`** a la matriz original.\n",
    "\n",
    "Esto es la base matemática de la **compresión con pérdida** (como en JPEG), el **filtrado de ruido** en datos, y los **sistemas de recomendación** (donde se aproxima la matriz gigante de usuario-ítem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo Demostrativo 2: Compresión de Imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DATOS: Una imagen es solo una matriz de valores de píxeles.\n",
    "img = imagen_mapache\n",
    "\n",
    "# 2. APLICACIÓN: Calculamos la SVD de la matriz de la imagen.\n",
    "U, s, Vt = np.linalg.svd(img)\n",
    "Sigma = np.diag(s) # Construimos la matriz diagonal Sigma\n",
    "\n",
    "print(f\"Dimensiones originales: U:{U.shape}, s:{s.shape}, Vt:{Vt.shape}\")\n",
    "\n",
    "# 3. RECONSTRUCCIÓN Y VISUALIZACIÓN: Reconstruimos la imagen usando diferentes números de valores singulares (k).\n",
    "fig, axs = plt.subplots(1, 5, figsize=(25, 5))\n",
    "ranks = [5, 20, 50, 100, len(s)]\n",
    "\n",
    "for i, k in enumerate(ranks):\n",
    "    # Tomamos las primeras k columnas de U, k valores singulares, y k filas de Vt\n",
    "    A_k = U[:, :k] @ Sigma[:k, :k] @ Vt[:k, :]\n",
    "    \n",
    "    # Calculamos el ratio de compresión (aproximado)\n",
    "    original_size = img.shape[0] * img.shape[1]\n",
    "    compressed_size = U[:, :k].size + s[:k].size + Vt[:k, :].size\n",
    "    ratio = compressed_size / original_size * 100\n",
    "    \n",
    "    axs[i].imshow(A_k, cmap='gray')\n",
    "    title = f'Original' if k == len(s) else f'Rango k = {k}'\n",
    "    axs[i].set_title(f'{title}\\n({ratio:.1f}% del tamaño)')\n",
    "    axs[i].axis('off')\n",
    "\n",
    "plt.suptitle('Compresión de Imagen usando SVD de Rango Reducido', fontsize=18, y=1.02); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Aplicación 2: La Pseudoinversa y Mínimos Cuadrados\n",
    "\n",
    "Cuando resolvimos $X\\vec{\\beta}=\\vec{y}$ con las Ecuaciones Normales, necesitábamos que $X^T X$ fuera invertible. Pero, ¿qué pasa si hay multicolinealidad perfecta y $X^T X$ es singular? La SVD nos da una solución más general y numéricamente estable: la **pseudoinversa de Moore-Penrose**.\n",
    "\n",
    "La pseudoinversa de $A = U \\Sigma V^T$ se calcula como $ A^+ = V \\Sigma^+ U^T $, donde $\\Sigma^+$ se obtiene de $\\Sigma$ tomando el recíproco ($1/\\sigma_i$) de los valores singulares **no nulos**.\n",
    "\n",
    "La solución de mínimos cuadrados de norma mínima para $A\\vec{x}=\\vec{b}$ es simplemente:\n",
    "$$ \\hat{x} = A^+ \\vec{b} $$\n",
    "Este método es el más robusto para resolver regresiones lineales, y es lo que funciones como `np.linalg.lstsq` usan internamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo Demostrativo 3: SVD para Regresión Robusta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DATOS: Usamos el dataset con multicolinealidad perfecta.\n",
    "X_raw = datos_multicolineales[['x1', 'x2', 'x3']].values\n",
    "X = np.c_[np.ones(X_raw.shape[0]), X_raw] # Añadimos intercepto\n",
    "y = datos_multicolineales['y'].values\n",
    "\n",
    "# 2. APLICACIÓN: Resolvemos usando la pseudoinversa.\n",
    "X_plus = np.linalg.pinv(X) # Calculamos la pseudoinversa\n",
    "beta_hat = X_plus @ y\n",
    "\n",
    "# 3. INTERPRETACIÓN\n",
    "print(\"Matriz X (con multicolinealidad perfecta):\")\n",
    "print(X[:5])\n",
    "print(f\"\\nNúmero de Condición de XᵀX: {np.linalg.cond(X.T @ X):.2e} (Extremadamente alto)\")\n",
    "print(\"\\nLas Ecuaciones Normales fallarían aquí.\")\n",
    "print(f\"\\nSolución de Mínimos Cuadrados vía Pseudoinversa (SVD):\")\n",
    "print(f\"β_hat = {np.round(beta_hat, 4)}\")\n",
    "print(\"\\nLa SVD encuentra una solución estable y de norma mínima incluso en casos de dependencia lineal perfecta.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conexión Final: SVD y PCA\n",
    "\n",
    "La conexión entre SVD y PCA es profunda y elegante. Mientras que PCA se define a través de la eigendescomposición de la matriz de covarianza, resulta que la SVD de la matriz de datos (centrada) nos da los mismos resultados de una forma numéricamente más estable.\n",
    "\n",
    "- Las **columnas de $V$** (los vectores singulares derechos) son los **Componentes Principales** (los eigenvectores de la matriz de covarianza).\n",
    "- Los **valores singulares ($s$) al cuadrado** son proporcionales a los **eigenvalores** de la matriz de covarianza (la varianza explicada por cada componente)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Ejercicios Guiados con Scaffolding (8+)\n",
    "Rellena las partes marcadas con `# COMPLETAR` para afianzar tu comprensión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 1: Descomponiendo una Matriz ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS\n",
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# TODO: Calcula la SVD de A.\n",
    "# PISTA: Usa np.linalg.svd().\n",
    "U, s, Vt = # COMPLETAR\n",
    "\n",
    "# VERIFICACIÓN\n",
    "assert U.shape == (2, 2)\n",
    "assert s.shape == (2,)\n",
    "assert Vt.shape == (3, 3)\n",
    "print(\"✅ ¡SVD calculada correctamente!\")\n",
    "print(f\"Forma de U: {U.shape}\")\n",
    "print(f\"Valores singulares s: {s}\")\n",
    "print(f\"Forma de Vᵀ: {Vt.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 2: Reconstruyendo desde SVD ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS: Los componentes U, s, Vt del ejercicio anterior.\n",
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "U, s, Vt = np.linalg.svd(A)\n",
    "\n",
    "# Para reconstruir, Sigma (Σ) debe tener la misma forma que A.\n",
    "# TODO 1: Crea una matriz de ceros con la forma de A.\n",
    "Sigma = # COMPLETAR\n",
    "\n",
    "# TODO 2: Rellena la diagonal de Sigma con los valores singulares 's'.\n",
    "# PISTA: Sigma[:A.shape[1], :A.shape[1]] = np.diag(s)\n",
    "# COMPLETAR\n",
    "\n",
    "# TODO 3: Reconstruye A multiplicando U @ Sigma @ Vt.\n",
    "A_reconstruida = # COMPLETAR\n",
    "\n",
    "# VERIFICACIÓN\n",
    "assert np.allclose(A, A_reconstruida)\n",
    "print(\"✅ ¡Matriz reconstruida con éxito!\")\n",
    "print(np.round(A_reconstruida, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 3: Aproximación de Rango 1 ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS: U, s, Vt de la matriz A.\n",
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "U, s, Vt = np.linalg.svd(A)\n",
    "k = 1 # Rango de la aproximación\n",
    "\n",
    "# TODO 1: Selecciona las primeras 'k' columnas de U.\n",
    "Uk = # COMPLETAR\n",
    "\n",
    "# TODO 2: Selecciona los primeros 'k' valores singulares y ponlos en una matriz diagonal.\n",
    "Sk = # COMPLETAR\n",
    "\n",
    "# TODO 3: Selecciona las primeras 'k' filas de Vt.\n",
    "Vtk = # COMPLETAR\n",
    "\n",
    "# TODO 4: Calcula la aproximación de rango k.\n",
    "A_k = # COMPLETAR\n",
    "\n",
    "assert A_k.shape == A.shape\n",
    "print(f\"✅ Aproximación de Rango {k} calculada:\")\n",
    "print(np.round(A_k, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 4: Resolver OLS con Pseudoinversa ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS: Matriz de diseño X y vector y del Hilo Conductor.\n",
    "X_feature = datos_estudiantes[['horas_estudio']].values\n",
    "X = np.c_[np.ones(X_feature.shape[0]), X_feature]\n",
    "y = datos_estudiantes['calificacion_examen'].values\n",
    "\n",
    "# TODO 1: Calcula la pseudoinversa de X.\n",
    "# PISTA: Usa np.linalg.pinv().\n",
    "X_plus = # COMPLETAR\n",
    "\n",
    "# TODO 2: Calcula la solución de mínimos cuadrados beta_hat.\n",
    "beta_hat = # COMPLETAR\n",
    "\n",
    "# VERIFICACIÓN\n",
    "beta_lstsq, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n",
    "assert np.allclose(beta_hat, beta_lstsq)\n",
    "print(\"✅ ¡OLS resuelto con pseudoinversa!\")\n",
    "print(f\"β_hat = {np.round(beta_hat, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# 5. Banco de Ejercicios Prácticos (30+)\n",
    "Ahora te toca a ti. Resuelve estos ejercicios para consolidar tu conocimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte A: Cálculo y Aproximación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A1 (🟢 Fácil):** Calcula la SVD de $A = \\begin{pmatrix} 3 & 0 \\\\ 0 & -2 \\end{pmatrix}$. ¿Qué son U, s y Vt en este caso simple?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A2 (🟢 Fácil):** Calcula la SVD de la matriz de rotación $R = \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix}$. ¿Qué valores singulares obtienes? ¿Tiene sentido?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A3 (🟡 Medio):** Dada la matriz $M = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix}$, calcula su SVD y su mejor aproximación de rango 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A4 (🟡 Medio):** Genera una matriz aleatoria de 100x50. Calcula su SVD y luego reconstrúyela usando solo los 10 valores singulares más grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A5 (🔴 Reto):** La norma de Frobenius de una matriz es $\\|A\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2}$. También es igual a la raíz cuadrada de la suma de los cuadrados de sus valores singulares. Genera una matriz 5x3 aleatoria y verifica esta propiedad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte B: Aplicaciones (Pseudoinversa y PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B1 (🟢 Fácil):** Usa `np.linalg.pinv` para calcular la pseudoinversa de $A = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$ (un vector columna)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B2 (🟡 Medio):** Usa `np.linalg.pinv` para calcular la pseudoinversa de la matriz singular $S = \\begin{pmatrix} 1 & 2 \\\\ 2 & 4 \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B3 (🟡 Medio):** Resuelve el problema de regresión simple para `datos_estudiantes` usando la pseudoinversa y verifica que los coeficientes coinciden con los de `lstsq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B4 (🔴 Reto):** Resuelve la regresión múltiple para el dataset `datos_multicolineales` usando la pseudoinversa. Compara tus coeficientes $\\beta$ con los que obtendrías de `lstsq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B5 (🔴 Reto):** Toma los datos de estudiantes centrados del Ejemplo 3. Proyecta los datos sobre el primer componente principal (la primera columna de V, o `Vt.T[:, 0]`). El resultado es la \"coordenada\" de cada estudiante a lo largo del eje más importante. Haz un histograma de estas coordenadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Mini-Quiz de Autoevaluación\n",
    "\n",
    "*Responde estas preguntas para verificar tu comprensión.*\n",
    "\n",
    "1. ¿Cuáles son las tres matrices que componen la SVD de una matriz A y qué representa geométricamente cada una?\n",
    "2. ¿Para qué tipo de problemas de regresión es la pseudoinversa (calculada vía SVD) particularmente útil?\n",
    "3. ¿Qué componentes de la SVD de una matriz de datos (centrada) corresponden a los Componentes Principales de PCA?\n",
    "4. Verdadero o Falso: La SVD solo se puede aplicar a matrices cuadradas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Próximos Pasos (Cierre del Área 1.1)\n",
    "\n",
    "**¡Felicidades, has llegado a la cima del Álgebra Lineal aplicada!** Con la SVD, tienes la herramienta más poderosa y robusta para analizar y manipular matrices. Has conectado todos los puntos, desde la definición de un vector hasta la implementación de PCA usando SVD, demostrando cómo la teoría abstracta potencia las aplicaciones más importantes de la ciencia de datos.\n",
    "\n",
    "- **Siguiente Parada:** **Área 1.2: Cálculo**. Cambiaremos de marcha para explorar la matemática del cambio. Descubriremos cómo las derivadas nos permiten encontrar la \"pendiente\" en espacios de alta dimensión, una idea fundamental para entrenar y optimizar casi todos los modelos de Machine Learning mediante algoritmos como el Descenso de Gradiente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
