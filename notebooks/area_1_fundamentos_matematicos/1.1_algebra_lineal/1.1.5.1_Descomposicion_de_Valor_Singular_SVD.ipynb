{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.5.1: Descomposici√≥n de Valor Singular (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos de Aprendizaje\n",
    "\n",
    "Al completar este notebook, ser√°s capaz de:\n",
    "\n",
    "- **Explicar** la SVD como la descomposici√≥n de **cualquier** matriz $A$ en $U \\Sigma V^T$.\n",
    "- **Interpretar** geom√©tricamente cada componente: Rotaci√≥n ($V^T$), Escalamiento ($\\Sigma$), y Rotaci√≥n ($U$).\n",
    "- **Utilizar** la SVD para construir **aproximaciones de rango reducido** para la compresi√≥n de datos.\n",
    "- **Resolver** problemas de m√≠nimos cuadrados de forma robusta usando la **Pseudo-inversa**, derivada de la SVD.\n",
    "- **Conectar** expl√≠citamente la SVD con el **An√°lisis de Componentes Principales (PCA)**, entendiendo por qu√© es el m√©todo num√©ricamente preferido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Celda de Configuraci√≥n (Oculta) ---\n",
    "%display latex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import datasets # Para la imagen de ejemplo\n",
    "\n",
    "def plot_svd_transformation(matrix):\n",
    "    t = np.linspace(0, 2*np.pi, 100)\n",
    "    circle = np.vstack((np.cos(t), np.sin(t)))\n",
    "    U, s, Vt = np.linalg.svd(matrix)\n",
    "    S = np.diag(s)\n",
    "    # Asegurarse que las matrices tengan las dimensiones correctas para 2D\n",
    "    if U.shape[0] > 2:\n",
    "        U = U[:, :2]\n",
    "    if Vt.shape[0] > 2:\n",
    "        Vt = Vt[:2, :]\n",
    "    if S.shape[0] > 2:\n",
    "        S = S[:2, :2]\n",
    "        \n",
    "    step1 = Vt @ circle\n",
    "    step2 = S @ step1\n",
    "    step3 = U @ step2\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 4, figsize=(22, 5.5))\n",
    "    titles = ['1. Original', '2. Rotaci√≥n por $V^T$', '3. Escalamiento por $\\Sigma$', '4. Rotaci√≥n por U (Final)']\n",
    "    data = [circle, step1, step2, step3]\n",
    "    \n",
    "    for i, ax in enumerate(axs):\n",
    "        ax.plot(data[i][0, :], data[i][1, :], lw=3)\n",
    "        ax.set_title(titles[i], fontsize=14)\n",
    "        ax.axis('equal'); ax.grid(True)\n",
    "        ax.axhline(0, color='black', lw=0.5); ax.axvline(0, color='black', lw=0.5)\n",
    "        \n",
    "    plt.suptitle('Descomposici√≥n Geom√©trica de una Transformaci√≥n v√≠a SVD', fontsize=18, y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## ‚öôÔ∏è El Arsenal de Datasets: Nuestra Fuente de Ejercicios\n",
    "\n",
    "La SVD es la \"navaja suiza\" del √°lgebra lineal. Para explorarla, necesitamos datasets que nos permitan ver sus superpoderes: compresi√≥n, soluci√≥n de sistemas y su conexi√≥n con PCA. Usaremos im√°genes, datos de estudiantes y matrices con propiedades especiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURACI√ìN DE DATASETS ===\n",
    "from src.data_generation.create_student_performance import create_student_performance_data\n",
    "from src.data_generation.create_edge_cases import create_edge_cases\n",
    "from src.data_generation.create_special_matrices import create_special_matrices\n",
    "\n",
    "# Configuraci√≥n centralizada de aleatoriedad para REPRODUCIBILIDAD\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "# === Generaci√≥n de Datasets y Matrices para este Notebook ===\n",
    "\n",
    "# üí° CONTEXTO PEDAG√ìGICO: Hilo Conductor (Aplicaci√≥n a PCA)\n",
    "# La conexi√≥n entre SVD y PCA es uno de los resultados m√°s importantes. Usaremos\n",
    "# nuestros datos de estudiantes para demostrar que la SVD de la matriz de datos nos da\n",
    "# directamente los componentes principales, de una forma m√°s estable que la eigendescomposici√≥n.\n",
    "datos_estudiantes = create_student_performance_data(rng, simplified=True, n_samples=200)\n",
    "\n",
    "# üí° CONTEXTO PEDAG√ìGICO: Compresi√≥n de Datos\n",
    "# Una imagen es solo una matriz de valores de p√≠xeles. Esto la convierte en el ejemplo\n",
    "# perfecto para visualizar la aproximaci√≥n de rango reducido, la base de la compresi√≥n.\n",
    "imagen_mapache = datasets.face(gray=True)\n",
    "\n",
    "# üí° CONTEXTO PEDAG√ìGICO: M√≠nimos Cuadrados Robusto\n",
    "# Usaremos un dataset con multicolinealidad para demostrar c√≥mo la pseudo-inversa,\n",
    "# calculada con SVD, puede resolver problemas de m√≠nimos cuadrados donde las Ecuaciones Normales fallan.\n",
    "datos_multicolineales = create_edge_cases(rng, case_type='multicollinear', n_samples=50)\n",
    "\n",
    "print(\"Datasets y matrices generados y listos para usar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. La Gran Intuici√≥n: Toda Matriz es Rotaci√≥n, Escalamiento y Rotaci√≥n\n",
    "\n",
    "La Descomposici√≥n de Valor Singular (SVD) es posiblemente el teorema m√°s importante del √°lgebra lineal aplicada. Afirma que **cualquier** matriz $A$ de $m \\times n$ (incluso no cuadrada) puede ser factorizada en el producto de tres matrices con interpretaciones geom√©tricas muy claras:\n",
    "$$ A = U \\Sigma V^T $$\n",
    "\n",
    "- **$V^T$ (Primera Rotaci√≥n):** Una matriz **ortogonal** ($n \\times n$) que rota el espacio de entrada sin cambiar longitudes ni √°ngulos. Alinea los ejes del espacio de entrada con los ejes principales de la transformaci√≥n.\n",
    "- **$\\Sigma$ (Escalamiento):** Una matriz **diagonal** ($m \\times n$) que estira o encoge el espacio a lo largo de los ejes rotados. Sus elementos diagonales $\\sigma_i$ son los **valores singulares** de A, y est√°n ordenados de mayor a menor ($\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0$).\n",
    "- **$U$ (Segunda Rotaci√≥n):** Otra matriz **ortogonal** ($m \\times m$) que rota el espacio de salida resultante a su orientaci√≥n final.\n",
    "\n",
    "La SVD nos da un desglose completo de la \"receta\" de cualquier transformaci√≥n lineal: una rotaci√≥n, un escalamiento puro a lo largo de ejes perpendiculares, y una rotaci√≥n final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo Demostrativo 1: Visualizaci√≥n Geom√©trica de la SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DATOS: Una matriz de transformaci√≥n 2x2 simple.\n",
    "A = np.array([[3, 0], [2, 2]])\n",
    "\n",
    "# 2. APLICACI√ìN Y VISUALIZACI√ìN\n",
    "# La funci√≥n de ayuda `plot_svd_transformation` calcula la SVD y aplica cada paso\n",
    "# a un c√≠rculo unitario para que podamos ver la descomposici√≥n de la acci√≥n.\n",
    "plot_svd_transformation(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Aplicaci√≥n 1: Aproximaci√≥n de Rango Reducido (Compresi√≥n)\n",
    "\n",
    "Los valores singulares en $\\Sigma$ est√°n ordenados por \"importancia\". El primer valor singular, $\\sigma_1$, captura la direcci√≥n de mayor acci√≥n de la matriz. Al quedarnos solo con los `k` valores singulares m√°s grandes, podemos reconstruir una matriz $A_k = U_k \\Sigma_k V_k^T$ que es la **mejor aproximaci√≥n posible de rango `k`** a la matriz original.\n",
    "\n",
    "Esto es la base matem√°tica de la **compresi√≥n con p√©rdida** (como en JPEG), el **filtrado de ruido** en datos, y los **sistemas de recomendaci√≥n** (donde se aproxima la matriz gigante de usuario-√≠tem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo Demostrativo 2: Compresi√≥n de Imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DATOS: Una imagen es solo una matriz de valores de p√≠xeles.\n",
    "img = imagen_mapache\n",
    "\n",
    "# 2. APLICACI√ìN: Calculamos la SVD de la matriz de la imagen.\n",
    "U, s, Vt = np.linalg.svd(img)\n",
    "Sigma = np.diag(s) # Construimos la matriz diagonal Sigma\n",
    "\n",
    "print(f\"Dimensiones originales: U:{U.shape}, s:{s.shape}, Vt:{Vt.shape}\")\n",
    "\n",
    "# 3. RECONSTRUCCI√ìN Y VISUALIZACI√ìN: Reconstruimos la imagen usando diferentes n√∫meros de valores singulares (k).\n",
    "fig, axs = plt.subplots(1, 5, figsize=(25, 5))\n",
    "ranks = [5, 20, 50, 100, len(s)]\n",
    "\n",
    "for i, k in enumerate(ranks):\n",
    "    # Tomamos las primeras k columnas de U, k valores singulares, y k filas de Vt\n",
    "    A_k = U[:, :k] @ Sigma[:k, :k] @ Vt[:k, :]\n",
    "    \n",
    "    # Calculamos el ratio de compresi√≥n (aproximado)\n",
    "    original_size = img.shape[0] * img.shape[1]\n",
    "    compressed_size = U[:, :k].size + s[:k].size + Vt[:k, :].size\n",
    "    ratio = compressed_size / original_size * 100\n",
    "    \n",
    "    axs[i].imshow(A_k, cmap='gray')\n",
    "    title = f'Original' if k == len(s) else f'Rango k = {k}'\n",
    "    axs[i].set_title(f'{title}\\n({ratio:.1f}% del tama√±o)')\n",
    "    axs[i].axis('off')\n",
    "\n",
    "plt.suptitle('Compresi√≥n de Imagen usando SVD de Rango Reducido', fontsize=18, y=1.02); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Aplicaci√≥n 2: La Pseudoinversa y M√≠nimos Cuadrados\n",
    "\n",
    "Cuando resolvimos $X\\vec{\\beta}=\\vec{y}$ con las Ecuaciones Normales, necesit√°bamos que $X^T X$ fuera invertible. Pero, ¬øqu√© pasa si hay multicolinealidad perfecta y $X^T X$ es singular? La SVD nos da una soluci√≥n m√°s general y num√©ricamente estable: la **pseudoinversa de Moore-Penrose**.\n",
    "\n",
    "La pseudoinversa de $A = U \\Sigma V^T$ se calcula como $ A^+ = V \\Sigma^+ U^T $, donde $\\Sigma^+$ se obtiene de $\\Sigma$ tomando el rec√≠proco ($1/\\sigma_i$) de los valores singulares **no nulos**.\n",
    "\n",
    "La soluci√≥n de m√≠nimos cuadrados de norma m√≠nima para $A\\vec{x}=\\vec{b}$ es simplemente:\n",
    "$$ \\hat{x} = A^+ \\vec{b} $$\n",
    "Este m√©todo es el m√°s robusto para resolver regresiones lineales, y es lo que funciones como `np.linalg.lstsq` usan internamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo Demostrativo 3: SVD para Regresi√≥n Robusta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DATOS: Usamos el dataset con multicolinealidad perfecta.\n",
    "X_raw = datos_multicolineales[['x1', 'x2', 'x3']].values\n",
    "X = np.c_[np.ones(X_raw.shape[0]), X_raw] # A√±adimos intercepto\n",
    "y = datos_multicolineales['y'].values\n",
    "\n",
    "# 2. APLICACI√ìN: Resolvemos usando la pseudoinversa.\n",
    "X_plus = np.linalg.pinv(X) # Calculamos la pseudoinversa\n",
    "beta_hat = X_plus @ y\n",
    "\n",
    "# 3. INTERPRETACI√ìN\n",
    "print(\"Matriz X (con multicolinealidad perfecta):\")\n",
    "print(X[:5])\n",
    "print(f\"\\nN√∫mero de Condici√≥n de X·µÄX: {np.linalg.cond(X.T @ X):.2e} (Extremadamente alto)\")\n",
    "print(\"\\nLas Ecuaciones Normales fallar√≠an aqu√≠.\")\n",
    "print(f\"\\nSoluci√≥n de M√≠nimos Cuadrados v√≠a Pseudoinversa (SVD):\")\n",
    "print(f\"Œ≤_hat = {np.round(beta_hat, 4)}\")\n",
    "print(\"\\nLa SVD encuentra una soluci√≥n estable y de norma m√≠nima incluso en casos de dependencia lineal perfecta.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conexi√≥n Final: SVD y PCA\n",
    "\n",
    "La conexi√≥n entre SVD y PCA es profunda y elegante. Mientras que PCA se define a trav√©s de la eigendescomposici√≥n de la matriz de covarianza, resulta que la SVD de la matriz de datos (centrada) nos da los mismos resultados de una forma num√©ricamente m√°s estable.\n",
    "\n",
    "- Las **columnas de $V$** (los vectores singulares derechos) son los **Componentes Principales** (los eigenvectores de la matriz de covarianza).\n",
    "- Los **valores singulares ($s$) al cuadrado** son proporcionales a los **eigenvalores** de la matriz de covarianza (la varianza explicada por cada componente)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Ejercicios Guiados con Scaffolding (8+)\n",
    "Rellena las partes marcadas con `# COMPLETAR` para afianzar tu comprensi√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 1: Descomponiendo una Matriz ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS\n",
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# TODO: Calcula la SVD de A.\n",
    "# PISTA: Usa np.linalg.svd().\n",
    "U, s, Vt = # COMPLETAR\n",
    "\n",
    "# VERIFICACI√ìN\n",
    "assert U.shape == (2, 2)\n",
    "assert s.shape == (2,)\n",
    "assert Vt.shape == (3, 3)\n",
    "print(\"‚úÖ ¬°SVD calculada correctamente!\")\n",
    "print(f\"Forma de U: {U.shape}\")\n",
    "print(f\"Valores singulares s: {s}\")\n",
    "print(f\"Forma de V·µÄ: {Vt.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 2: Reconstruyendo desde SVD ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS: Los componentes U, s, Vt del ejercicio anterior.\n",
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "U, s, Vt = np.linalg.svd(A)\n",
    "\n",
    "# Para reconstruir, Sigma (Œ£) debe tener la misma forma que A.\n",
    "# TODO 1: Crea una matriz de ceros con la forma de A.\n",
    "Sigma = # COMPLETAR\n",
    "\n",
    "# TODO 2: Rellena la diagonal de Sigma con los valores singulares 's'.\n",
    "# PISTA: Sigma[:A.shape[1], :A.shape[1]] = np.diag(s)\n",
    "# COMPLETAR\n",
    "\n",
    "# TODO 3: Reconstruye A multiplicando U @ Sigma @ Vt.\n",
    "A_reconstruida = # COMPLETAR\n",
    "\n",
    "# VERIFICACI√ìN\n",
    "assert np.allclose(A, A_reconstruida)\n",
    "print(\"‚úÖ ¬°Matriz reconstruida con √©xito!\")\n",
    "print(np.round(A_reconstruida, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 3: Aproximaci√≥n de Rango 1 ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS: U, s, Vt de la matriz A.\n",
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "U, s, Vt = np.linalg.svd(A)\n",
    "k = 1 # Rango de la aproximaci√≥n\n",
    "\n",
    "# TODO 1: Selecciona las primeras 'k' columnas de U.\n",
    "Uk = # COMPLETAR\n",
    "\n",
    "# TODO 2: Selecciona los primeros 'k' valores singulares y ponlos en una matriz diagonal.\n",
    "Sk = # COMPLETAR\n",
    "\n",
    "# TODO 3: Selecciona las primeras 'k' filas de Vt.\n",
    "Vtk = # COMPLETAR\n",
    "\n",
    "# TODO 4: Calcula la aproximaci√≥n de rango k.\n",
    "A_k = # COMPLETAR\n",
    "\n",
    "assert A_k.shape == A.shape\n",
    "print(f\"‚úÖ Aproximaci√≥n de Rango {k} calculada:\")\n",
    "print(np.round(A_k, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 4: Resolver OLS con Pseudoinversa ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS: Matriz de dise√±o X y vector y del Hilo Conductor.\n",
    "X_feature = datos_estudiantes[['horas_estudio']].values\n",
    "X = np.c_[np.ones(X_feature.shape[0]), X_feature]\n",
    "y = datos_estudiantes['calificacion_examen'].values\n",
    "\n",
    "# TODO 1: Calcula la pseudoinversa de X.\n",
    "# PISTA: Usa np.linalg.pinv().\n",
    "X_plus = # COMPLETAR\n",
    "\n",
    "# TODO 2: Calcula la soluci√≥n de m√≠nimos cuadrados beta_hat.\n",
    "beta_hat = # COMPLETAR\n",
    "\n",
    "# VERIFICACI√ìN\n",
    "beta_lstsq, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n",
    "assert np.allclose(beta_hat, beta_lstsq)\n",
    "print(\"‚úÖ ¬°OLS resuelto con pseudoinversa!\")\n",
    "print(f\"Œ≤_hat = {np.round(beta_hat, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# 5. Banco de Ejercicios Pr√°cticos (30+)\n",
    "Ahora te toca a ti. Resuelve estos ejercicios para consolidar tu conocimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte A: C√°lculo y Aproximaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A1 (üü¢ F√°cil):** Calcula la SVD de $A = \\begin{pmatrix} 3 & 0 \\\\ 0 & -2 \\end{pmatrix}$. ¬øQu√© son U, s y Vt en este caso simple?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A2 (üü¢ F√°cil):** Calcula la SVD de la matriz de rotaci√≥n $R = \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix}$. ¬øQu√© valores singulares obtienes? ¬øTiene sentido?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A3 (üü° Medio):** Dada la matriz $M = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix}$, calcula su SVD y su mejor aproximaci√≥n de rango 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A4 (üü° Medio):** Genera una matriz aleatoria de 100x50. Calcula su SVD y luego reconstr√∫yela usando solo los 10 valores singulares m√°s grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A5 (üî¥ Reto):** La norma de Frobenius de una matriz es $\\|A\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2}$. Tambi√©n es igual a la ra√≠z cuadrada de la suma de los cuadrados de sus valores singulares. Genera una matriz 5x3 aleatoria y verifica esta propiedad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte B: Aplicaciones (Pseudoinversa y PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B1 (üü¢ F√°cil):** Usa `np.linalg.pinv` para calcular la pseudoinversa de $A = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$ (un vector columna)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B2 (üü° Medio):** Usa `np.linalg.pinv` para calcular la pseudoinversa de la matriz singular $S = \\begin{pmatrix} 1 & 2 \\\\ 2 & 4 \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B3 (üü° Medio):** Resuelve el problema de regresi√≥n simple para `datos_estudiantes` usando la pseudoinversa y verifica que los coeficientes coinciden con los de `lstsq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B4 (üî¥ Reto):** Resuelve la regresi√≥n m√∫ltiple para el dataset `datos_multicolineales` usando la pseudoinversa. Compara tus coeficientes $\\beta$ con los que obtendr√≠as de `lstsq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B5 (üî¥ Reto):** Toma los datos de estudiantes centrados del Ejemplo 3. Proyecta los datos sobre el primer componente principal (la primera columna de V, o `Vt.T[:, 0]`). El resultado es la \"coordenada\" de cada estudiante a lo largo del eje m√°s importante. Haz un histograma de estas coordenadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Mini-Quiz de Autoevaluaci√≥n\n",
    "\n",
    "*Responde estas preguntas para verificar tu comprensi√≥n.*\n",
    "\n",
    "1. ¬øCu√°les son las tres matrices que componen la SVD de una matriz A y qu√© representa geom√©tricamente cada una?\n",
    "2. ¬øPara qu√© tipo de problemas de regresi√≥n es la pseudoinversa (calculada v√≠a SVD) particularmente √∫til?\n",
    "3. ¬øQu√© componentes de la SVD de una matriz de datos (centrada) corresponden a los Componentes Principales de PCA?\n",
    "4. Verdadero o Falso: La SVD solo se puede aplicar a matrices cuadradas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Pr√≥ximos Pasos (Cierre del √Årea 1.1)\n",
    "\n",
    "**¬°Felicidades, has llegado a la cima del √Ålgebra Lineal aplicada!** Con la SVD, tienes la herramienta m√°s poderosa y robusta para analizar y manipular matrices. Has conectado todos los puntos, desde la definici√≥n de un vector hasta la implementaci√≥n de PCA usando SVD, demostrando c√≥mo la teor√≠a abstracta potencia las aplicaciones m√°s importantes de la ciencia de datos.\n",
    "\n",
    "- **Siguiente Parada:** **√Årea 1.2: C√°lculo**. Cambiaremos de marcha para explorar la matem√°tica del cambio. Descubriremos c√≥mo las derivadas nos permiten encontrar la \"pendiente\" en espacios de alta dimensi√≥n, una idea fundamental para entrenar y optimizar casi todos los modelos de Machine Learning mediante algoritmos como el Descenso de Gradiente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
