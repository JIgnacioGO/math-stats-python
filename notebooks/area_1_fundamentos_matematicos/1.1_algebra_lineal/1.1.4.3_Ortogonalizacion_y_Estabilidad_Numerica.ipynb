{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.4.3: Ortogonalización y Estabilidad Numérica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos de Aprendizaje\n",
    "\n",
    "Al completar este notebook, serás capaz de:\n",
    "\n",
    "- **Diagnosticar** la inestabilidad de una matriz usando el **Número de Condición**.\n",
    "- **Entender** por qué las **bases ortonormales** son el \"santo grial\" para la estabilidad numérica en problemas de mínimos cuadrados.\n",
    "- **Aplicar** el proceso de **Ortogonalización de Gram-Schmidt** para crear una base ortonormal a partir de una base cualquiera.\n",
    "- **Utilizar** la **Descomposición QR** como la encarnación computacionalmente eficiente de Gram-Schmidt.\n",
    "- **Resolver** problemas de Mínimos Cuadrados de forma robusta usando la Descomposición QR y comparar su estabilidad con el método de las Ecuaciones Normales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Celda de Configuración (Oculta) ---\n",
    "%display latex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "def plot_gram_schmidt(v1, v2):\n",
    "    proj_v2_on_v1 = (v2.dot(v1) / v1.dot(v1)) * v1\n",
    "    u2 = v2 - proj_v2_on_v1\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    # Vectores originales\n",
    "    plt.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='#0072B2', width=0.015, label='v1 (base original)')\n",
    "    plt.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='#D55E00', width=0.015, label='v2 (base original)')\n",
    "    # Proceso\n",
    "    plt.quiver(0, 0, proj_v2_on_v1[0], proj_v2_on_v1[1], angles='xy', scale_units='xy', scale=1, color='gray', linestyle='--', label='proy_v1(v2)')\n",
    "    plt.quiver(proj_v2_on_v1[0], proj_v2_on_v1[1], u2[0], u2[1], angles='xy', scale_units='xy', scale=1, color='#009E73', width=0.015, label='u2 (componente ortogonal)')\n",
    "    \n",
    "    limit = np.max(np.abs(np.vstack((v1, v2, u2)))) * 1.2\n",
    "    plt.xlim(-limit, limit); plt.ylim(-limit, limit)\n",
    "    plt.grid(True); plt.legend(); plt.title('Visualización del Proceso de Gram-Schmidt')\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## ⚙️ El Arsenal de Datasets: Nuestra Fuente de Ejercicios\n",
    "\n",
    "La estabilidad numérica es un problema del mundo real. Para estudiarlo, necesitamos datasets que imiten los problemas que encontramos en la práctica, como la multicolinealidad. También usaremos vectores simples para visualizar la geometría de la ortogonalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURACIÓN DE DATASETS ===\n",
    "from src.data_generation.create_student_performance import create_student_performance_data\n",
    "from src.data_generation.create_edge_cases import create_edge_cases\n",
    "\n",
    "# Configuración centralizada de aleatoriedad para REPRODUCIBILIDAD\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "# === Generación de Datasets y Matrices para este Notebook ===\n",
    "\n",
    "# 💡 CONTEXTO PEDAGÓGICO: Hilo Conductor (Regresión Robusta)\n",
    "# Usaremos el dataset de estudiantes para comparar la solución de OLS obtenida con las\n",
    "# Ecuaciones Normales vs. la obtenida con la Descomposición QR, demostrando que en\n",
    "# casos bien condicionados, ambas coinciden.\n",
    "datos_estudiantes = create_student_performance_data(rng, simplified=True, n_samples=100)\n",
    "\n",
    "# 💡 CONTEXTO PEDAGÓGICO: El Caso Problema (Multicolinealidad)\n",
    "# La mejor forma de apreciar la estabilidad de QR es usarlo donde las Ecuaciones Normales\n",
    "# sufren. Este dataset con alta multicolinealidad será nuestro campo de pruebas.\n",
    "datos_multicolineales = create_edge_cases(rng, case_type='multicollinear', n_samples=100)\n",
    "\n",
    "# 💡 CONTEXTO PEDAGÓGICO: Vectores para Geometría\n",
    "# Para visualizar Gram-Schmidt, necesitamos vectores que NO sean ortogonales, para que\n",
    "# el algoritmo tenga algo que corregir.\n",
    "v1 = np.array([3, 1])\n",
    "v2 = np.array([2, 2])\n",
    "\n",
    "print(\"Datasets y vectores generados y listos para usar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. El Problema: Cuando las Ecuaciones Normales son Inestables\n",
    "\n",
    "En el notebook anterior, derivamos las Ecuaciones Normales $X^T X \\hat{\\beta} = X^T \\vec{y}$. Esta fórmula es teóricamente correcta, pero tiene un talón de Aquiles numérico: la matriz $X^T X$.\n",
    "\n",
    "Si las columnas de $X$ son casi linealmente dependientes (es decir, hay **multicolinealidad**), la matriz $X^T X$ se vuelve **mal condicionada**. Esto significa que es muy sensible a pequeños errores de redondeo en los cálculos de punto flotante, y la solución $\\hat{\\beta}$ puede ser muy imprecisa.\n",
    "\n",
    "#### Diagnóstico: El Número de Condición\n",
    "El **Número de Condición** de una matriz, `np.linalg.cond(A)`, mide esta sensibilidad. Un número cercano a 1 es ideal (como en una matriz ortogonal). Un número muy grande (e.g., > 1,000) es una bandera roja 🚩 de inestabilidad.\n",
    "\n",
    "**Regla Clave:** `cond(XᵀX) = cond(X)²`. Formar la matriz $X^T X$ **eleva al cuadrado el número de condición**, empeorando drásticamente la estabilidad del problema. Por esto, los métodos modernos evitan formar $X^T X$ explícitamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo Demostrativo 1: El Número de Condición Explota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DATOS: Usamos el dataset con multicolinealidad, donde x2 y x3 dependen de x1.\n",
    "X_mal_condicionado = datos_multicolineales[['x1', 'x2', 'x3']].values\n",
    "X_bien_condicionado = datos_estudiantes[['horas_estudio', 'calificacion_examen']].values\n",
    "\n",
    "# 2. APLICACIÓN: Comparamos los números de condición.\n",
    "cond_X_bien = np.linalg.cond(X_bien_condicionado)\n",
    "cond_XTX_bien = np.linalg.cond(X_bien_condicionado.T @ X_bien_condicionado)\n",
    "\n",
    "cond_X_mal = np.linalg.cond(X_mal_condicionado)\n",
    "cond_XTX_mal = np.linalg.cond(X_mal_condicionado.T @ X_mal_condicionado)\n",
    "\n",
    "# 3. INTERPRETACIÓN\n",
    "print(\"--- Caso Bien Condicionado ---\")\n",
    "print(f\"Número de Condición de X: {cond_X_bien:.2f}\")\n",
    "print(f\"Número de Condición de XᵀX: {cond_XTX_bien:.2f}\")\n",
    "\n",
    "print(\"\\n--- Caso Mal Condicionado (Multicolinealidad) ---\")\n",
    "print(f\"Número de Condición de X: {cond_X_mal:,.2f}\")\n",
    "print(f\"Número de Condición de XᵀX: {cond_XTX_mal:,.2f} (¡Inmanejablemente alto!)\")\n",
    "\n",
    "print(f\"\\nObserva que {cond_X_mal**2:,.2f} es aproximadamente igual a {cond_XTX_mal:,.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. La Solución: Ortogonalización\n",
    "\n",
    "El problema es que las columnas de $X$ \"se parecen\" entre sí. La solución es encontrar una **base ortonormal** para el espacio columna de $X$. Una base ortonormal está compuesta por vectores que son:\n",
    "1.  **Ortogonales:** Todos son perpendiculares entre sí (producto punto es cero).\n",
    "2.  **Normales:** Todos tienen longitud 1 (norma L2 es uno).\n",
    "\n",
    "Si pudiéramos reemplazar nuestra matriz $X$ por una matriz $Q$ con columnas ortonormales, las Ecuaciones Normales se simplificarían a $I \\hat{\\beta} = Q^T \\vec{y}$, una solución trivial y perfectamente estable.\n",
    "\n",
    "### El Algoritmo: Ortogonalización de Gram-Schmidt\n",
    "Este es el algoritmo clásico para construir una base ortogonal $\\{\\vec{u_1}, \\vec{u_2}, \\dots\\}$ a partir de una base cualquiera $\\{\\vec{v_1}, \\vec{v_2}, \\dots\\}$.\n",
    "- **Paso 1:** $\\vec{u_1} = \\vec{v_1}$\n",
    "- **Paso 2:** Se toma $\\vec{v_2}$ y se le resta su proyección sobre $\\vec{u_1}$. Lo que queda es ortogonal a $\\vec{u_1}$.\n",
    "  $$ \\vec{u_2} = \\vec{v_2} - \\text{proj}_{\\vec{u_1}}(\\vec{v_2}) $$\n",
    "- **Paso 3 (y siguientes):** Para $\\vec{v_k}$, se le restan sus proyecciones sobre todos los vectores ortogonales ya encontrados ($\\{\\vec{u_1}, \\dots, \\vec{u_{k-1}}\\}$).\n",
    "- **Normalización Final:** Se divide cada vector $\\vec{u_i}$ por su norma para obtener una base ortonormal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo Demostrativo 2: Visualización de Gram-Schmidt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DATOS: Dos vectores linealmente independientes pero no ortogonales.\n",
    "v1 = np.array([3, 1])\n",
    "v2 = np.array([2, 2])\n",
    "print(f\"Producto punto original (v1·v2): {v1 @ v2} (No es cero)\")\n",
    "\n",
    "# 2. APLICACIÓN: Aplicamos el proceso\n",
    "u1 = v1\n",
    "proj_v2_on_u1 = (v2 @ u1) / (u1 @ u1) * u1\n",
    "u2 = v2 - proj_v2_on_u1\n",
    "\n",
    "# 3. INTERPRETACIÓN Y VERIFICACIÓN\n",
    "print(f\"Nueva base ortogonal: u1={np.round(u1, 2)}, u2={np.round(u2, 2)}\")\n",
    "print(f\"Producto punto nuevo (u1·u2): {u1 @ u2:.10f} (Es cero)\")\n",
    "\n",
    "# 4. VISUALIZACIÓN\n",
    "plot_gram_schmidt(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. La Herramienta Práctica: Descomposición QR\n",
    "\n",
    "En la práctica, no implementamos Gram-Schmidt a mano (es numéricamente inestable para muchas columnas). Usamos la **Descomposición QR**, que es la encarnación de este proceso, implementada de forma robusta. Descompone cualquier matriz $A$ de $m \\times n$ en el producto:\n",
    "$$ A = QR $$\n",
    "- **$Q$**: Una matriz $m \\times n$ con **columnas ortonormales** que forman una base para el Espacio Columna de A.\n",
    "- **$R$**: Una **matriz triangular superior** $n \\times n$ que \"registra\" cómo se combinaron los vectores de Q para reconstruir A.\n",
    "\n",
    "### Resolviendo Mínimos Cuadrados con QR\n",
    "Sustituimos $X=QR$ en el problema de mínimos cuadrados $X\\vec{\\beta} = \\vec{y}$:\n",
    "$$ QR\\vec{\\beta} = \\vec{y} $$\n",
    "Multiplicamos por $Q^T$. Como $Q^T Q = I$, obtenemos:\n",
    "$$ R\\hat{\\beta} = Q^T\\vec{y} $$\n",
    "Como R es triangular, este sistema es muy fácil y **numéricamente estable** de resolver para $\\hat{\\beta}$ usando un método llamado \"sustitución hacia atrás\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo Demostrativo 3: Comparando OLS: Ecuaciones Normales vs. Descomposición QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DATOS: Usaremos el dataset con alta multicolinealidad.\n",
    "X = datos_multicolineales[['x1', 'x2', 'x3']].values\n",
    "y = datos_multicolineales['y'].values\n",
    "\n",
    "# 2. APLICACIÓN: Resolvemos de ambas formas.\n",
    "\n",
    "# Método 1: Ecuaciones Normales (Potencialmente Inestable)\n",
    "XTX = X.T @ X\n",
    "XTy = X.T @ y\n",
    "try:\n",
    "    beta_norm = np.linalg.solve(XTX, XTy)\n",
    "except np.linalg.LinAlgError:\n",
    "    beta_norm = [np.nan, np.nan, np.nan] # Falló\n",
    "\n",
    "# Método 2: Descomposición QR (Robusto)\n",
    "Q, R = np.linalg.qr(X)\n",
    "QTy = Q.T @ y\n",
    "beta_qr = np.linalg.solve(R, QTy)\n",
    "\n",
    "# 3. INTERPRETACIÓN Y COMPARACIÓN\n",
    "print(f\"Número de Condición de XᵀX: {np.linalg.cond(XTX):,.2f}\")\n",
    "print(f\"\\nSolución con Ecuaciones Normales: {np.round(beta_norm, 4)}\")\n",
    "print(f\"Solución con Descomposición QR: {np.round(beta_qr, 4)}\")\n",
    "print(\"\\nConclusión: La descomposición QR proporciona una solución estable incluso cuando las Ecuaciones Normales fallan o son imprecisas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Ejercicios Guiados con Scaffolding (8+)\n",
    "Rellena las partes marcadas con `# COMPLETAR` para afianzar tu comprensión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 1: Un Paso de Gram-Schmidt ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS\n",
    "v1 = np.array([1, 1, 0])\n",
    "v2 = np.array([1, 3, 1])\n",
    "\n",
    "# Nuestro primer vector ortogonal es v1.\n",
    "u1 = v1\n",
    "\n",
    "# TODO 1: Calcula la proyección de v2 sobre u1.\n",
    "proj_v2_on_u1 = # COMPLETAR\n",
    "\n",
    "# TODO 2: Calcula el segundo vector ortogonal, u2, restando la proyección de v2.\n",
    "u2 = # COMPLETAR\n",
    "\n",
    "# VERIFICACIÓN\n",
    "assert np.allclose(proj_v2_on_u1, np.array([2, 2, 0]))\n",
    "assert np.isclose(u1 @ u2, 0), \"Los vectores u1 y u2 no son ortogonales.\"\n",
    "print(\"✅ ¡Paso de Gram-Schmidt completado con éxito!\")\n",
    "print(f\"El componente de v2 ortogonal a v1 es: {u2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 2: Normalización ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS: El vector u2 del ejercicio anterior.\n",
    "u2 = np.array([-1, 1, 1])\n",
    "\n",
    "# TODO: Normaliza el vector u2 (divídelo por su norma L2).\n",
    "e2 = # COMPLETAR\n",
    "\n",
    "# VERIFICACIÓN\n",
    "assert np.isclose(np.linalg.norm(e2), 1.0), \"El vector no tiene norma 1.\"\n",
    "print(\"✅ ¡Vector normalizado correctamente!\")\n",
    "print(f\"Vector unitario e2: {np.round(e2, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 3: Realizar la Descomposición QR ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS\n",
    "A = np.array([[1, 1], [1, 2], [1, 3]])\n",
    "\n",
    "# TODO: Usa np.linalg.qr() para descomponer A en Q y R.\n",
    "Q, R = # COMPLETAR\n",
    "\n",
    "# VERIFICACIÓN\n",
    "assert Q.shape == (3, 2) and R.shape == (2, 2), \"Las formas de Q y R son incorrectas.\"\n",
    "assert np.allclose(Q.T @ Q, np.identity(2)), \"Las columnas de Q no son ortonormales.\"\n",
    "assert np.allclose(A, Q @ R), \"La reconstrucción Q@R no es igual a A.\"\n",
    "print(\"✅ ¡Descomposición QR exitosa!\")\n",
    "print(f\"Matriz Q (columnas ortonormales):\\n{np.round(Q, 3)}\")\n",
    "print(f\"\\nMatriz R (triangular superior):\\n{np.round(R, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 4: Resolver Mínimos Cuadrados con QR ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS\n",
    "X = np.array([[1, 1], [1, 2], [1, 3]])\n",
    "y = np.array([1, 2, 2])\n",
    "\n",
    "# Ya tenemos Q y R de X del ejercicio anterior.\n",
    "Q, R = np.linalg.qr(X)\n",
    "\n",
    "# El problema a resolver es R·β = Qᵀ·y\n",
    "\n",
    "# TODO 1: Calcula el lado derecho de la ecuación, Qᵀ·y.\n",
    "QTy = # COMPLETAR\n",
    "\n",
    "# TODO 2: Resuelve el sistema triangular para encontrar beta.\n",
    "beta_hat_qr = # COMPLETAR\n",
    "\n",
    "# VERIFICACIÓN\n",
    "beta_hat_norm = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "assert np.allclose(beta_hat_qr, beta_hat_norm)\n",
    "print(\"✅ ¡Sistema resuelto con QR!\")\n",
    "print(f\"β_hat = {np.round(beta_hat_qr, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# 5. Banco de Ejercicios Prácticos (30+)\n",
    "Ahora te toca a ti. Resuelve estos ejercicios para consolidar tu conocimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte A: Estabilidad y Gram-Schmidt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A1 (🟢 Fácil):** Calcula el número de condición de $A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}$ y de $A^TA$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A2 (🟢 Fácil):** Aplica el primer paso de Gram-Schmidt (encontrar $\\vec{u_2}$) a los vectores $\\vec{v_1} = [1, 1]$ y $\\vec{v_2} = [0, 2]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A3 (🟡 Medio):** Completa el proceso de Gram-Schmidt para los vectores del ejercicio A2 para encontrar una base ortonormal $\\{\\vec{e_1}, \\vec{e_2}\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A4 (🔴 Reto):** Aplica el proceso de Gram-Schmidt a los 3 vectores: $\\vec{v_1}=[1,1,0], \\vec{v_2}=[1,0,1], \\vec{v_3}=[0,1,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte B: Descomposición QR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B1 (🟢 Fácil):** Calcula la descomposición QR de la matriz $A = \\begin{pmatrix} 1 & 2 \\\\ 0 & 3 \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B2 (🟢 Fácil):** Verifica que para la matriz Q del ejercicio B1, se cumple que $Q^T Q$ es (aproximadamente) la matriz identidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B3 (🟡 Medio):** Usando `datos_estudiantes`, construye la matriz de diseño $X$ y calcula su descomposición QR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B4 (🔴 Reto):** Resuelve el problema de regresión del **Hilo Conductor** usando la Descomposición QR. Los pasos son:\n",
    "1. Construye la matriz de diseño `X` y el vector `y`.\n",
    "2. Calcula `Q, R = np.linalg.qr(X)`.\n",
    "3. Calcula `Qty = Q.T @ y`.\n",
    "4. Resuelve el sistema triangular `R @ beta = Qty` para `beta` usando `np.linalg.solve`.\n",
    "5. Compara los coeficientes con los obtenidos con las Ecuaciones Normales en el notebook anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Mini-Quiz de Autoevaluación\n",
    "\n",
    "*Responde estas preguntas para verificar tu comprensión.*\n",
    "\n",
    "1. ¿Por qué es problemático usar las Ecuaciones Normales cuando una matriz tiene un alto número de condición?\n",
    "2. ¿Qué propiedad clave tienen las columnas de la matriz Q en una descomposición QR?\n",
    "3. ¿Qué algoritmo teórico es la base de la descomposición QR?\n",
    "4. Al resolver $R\\hat{\\beta} = Q^T\\vec{y}$, ¿por qué es computacionalmente \"fácil\" encontrar $\\hat{\\beta}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Próximos Pasos\n",
    "\n",
    "Has aprendido sobre la importancia de la estabilidad numérica y cómo la ortogonalización nos proporciona métodos más robustos para resolver problemas de la vida real. Este es el estándar de oro para resolver problemas de mínimos cuadrados en software profesional.\n",
    "\n",
    "- En el notebook **`1.1.5.1_Descomposicion_de_Valor_Singular_SVD.ipynb`**, exploraremos la SVD, la \"navaja suiza\" de las descomposiciones matriciales. Es el método más robusto y general de todos, capaz de resolver sistemas de mínimos cuadrados incluso cuando las columnas de X son linealmente dependientes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
