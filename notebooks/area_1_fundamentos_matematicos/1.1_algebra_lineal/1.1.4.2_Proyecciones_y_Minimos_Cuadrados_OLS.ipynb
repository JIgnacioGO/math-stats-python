{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.4.2: Proyecciones y Mínimos Cuadrados Ordinarios (OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos de Aprendizaje\n",
    "\n",
    "Al completar este notebook, serás capaz de:\n",
    "\n",
    "- **Interpretar** geométricamente la solución de mínimos cuadrados como la **proyección ortogonal** de un vector sobre un subespacio.\n",
    "- **Derivar y entender** las **Ecuaciones Normales**: $X^T X \\hat{\\beta} = X^T \\vec{y}$, la base del OLS.\n",
    "- **Verificar** la condición fundamental de ortogonalidad del vector de error ($X^T \\vec{e} = \\vec{0}$).\n",
    "- **Construir** y resolver un modelo de regresión lineal 'desde cero' usando este método, en lugar de funciones de alto nivel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Celda de Configuración (Oculta) ---\n",
    "%display latex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## ⚙️ El Arsenal de Datasets: Nuestra Fuente de Ejercicios\n",
    "\n",
    "Para este notebook, que se sumerge en la mecánica interna de OLS, usaremos datasets que nos permitan visualizar claramente la geometría de las proyecciones y verificar las propiedades matemáticas de la solución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURACIÓN DE DATASETS ===\n",
    "from src.data_generation.create_student_performance import create_student_performance_data\n",
    "from src.data_generation.create_business_data import create_business_data\n",
    "from src.data_generation.create_geometric_shapes import create_geometric_shapes\n",
    "\n",
    "# Configuración centralizada de aleatoriedad para REPRODUCIBILIDAD\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "# === Generación de Datasets y Matrices para este Notebook ===\n",
    "\n",
    "# 💡 CONTEXTO PEDAGÓGICO: Hilo Conductor (Regresión OLS)\n",
    "# Usaremos el dataset de estudiantes para implementar la solución de OLS a través de las\n",
    "# Ecuaciones Normales, verificando que obtenemos el mismo resultado que con `lstsq`.\n",
    "datos_estudiantes = create_student_performance_data(rng, simplified=True, n_samples=100)\n",
    "\n",
    "# 💡 CONTEXTO PEDAGÓGICO: Geometría de la Proyección\n",
    "# Para visualizar la idea de proyectar un vector sobre un plano, crearemos un ejemplo\n",
    "# simple y de baja dimensión donde podamos graficar todos los elementos: el espacio, el vector y su proyección.\n",
    "X_plano = create_geometric_shapes(rng, 'line', n_samples=10).values # Generamos una base para un plano\n",
    "X_plano = np.c_[X_plano, X_plano[:, 0] - X_plano[:, 1]] # Creamos un plano en R^3\n",
    "\n",
    "# 💡 CONTEXTO PEDAGÓGICO: Regresión Múltiple con Ecuaciones Normales\n",
    "# Demostraremos que la belleza de las Ecuaciones Normales es que funcionan igual de bien\n",
    "# para la regresión múltiple, usando el dataset de negocio.\n",
    "datos_negocio = create_business_data(rng, n_samples=150)\n",
    "\n",
    "print(\"Datasets generados y listos para usar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. La Geometría de la \"Mejor Solución\"\n",
    "\n",
    "En el notebook anterior, aceptamos que `np.linalg.lstsq` nos daba la \"mejor solución\" a un sistema inconsistente $X\\vec{\\beta} = \\vec{y}$. Pero, ¿qué significa \"mejor\" geométricamente?\n",
    "\n",
    "1.  **El Problema:** El vector de nuestros datos reales, $\\vec{y}$ (las calificaciones), no se encuentra en el subespacio generado por las columnas de $X$, que llamamos **Espacio Columna** de $X$, o $\\text{Col}(X)$. $\\text{Col}(X)$ representa el universo de todas las posibles predicciones lineales que podemos hacer.\n",
    "\n",
    "2.  **La Solución:** Si no podemos alcanzar $\\vec{y}$ exactamente, la \"mejor solución\" es el vector $\\hat{y}$ (o $\\vec{p}$) *dentro* de $\\text{Col}(X)$ que está **más cerca** de $\\vec{y}$. Este punto es la **proyección ortogonal** de $\\vec{y}$ sobre el subespacio $\\text{Col}(X)$.\n",
    "\n",
    "**La Gran Intuición:** La distancia más corta de un punto (la punta de $\\vec{y}$) a un plano ($\\\text{Col}(X)$) se logra con una línea perpendicular. Esto significa que el **vector de error**, $\\vec{e} = \\vec{y} - \\hat{y}$, debe ser **ortogonal** a *todo* el espacio columna de $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo Demostrativo 1: Visualizando la Proyección Ortogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DATOS: Creamos un \"plano\" (espacio columna de X) y un vector \"y\" que no está en el plano.\n",
    "X = np.array([[1, 0, 1], [0, 1, 1], [1, 1, 2], [1, -1, 0]]) # X es 4x3 pero tiene rango 2\n",
    "y = np.array([4, 5, 10, 2])\n",
    "\n",
    "# 2. APLICACIÓN: Encontramos la proyección de y sobre el espacio columna de X usando lstsq.\n",
    "beta_hat, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n",
    "y_hat = X @ beta_hat # Esta es la proyección 'p'\n",
    "error_vec = y - y_hat\n",
    "\n",
    "# 3. INTERPRETACIÓN Y VERIFICACIÓN\n",
    "print(f\"Vector y (original): {y}\")\n",
    "print(f\"Vector ŷ (proyección): {np.round(y_hat, 2)}\")\n",
    "print(f\"Vector de error (e): {np.round(error_vec, 2)}\")\n",
    "\n",
    "# Verificamos que el error es ortogonal a las columnas de X\n",
    "orto_check = X.T @ error_vec\n",
    "print(f\"\\nVerificación de Ortogonalidad (X.T @ e): {np.round(orto_check, 9)}\")\n",
    "print(\"Los valores son prácticamente cero, confirmando la ortogonalidad.\")\n",
    "\n",
    "# (La visualización 3D/4D es compleja, pero el principio numérico se mantiene)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. La Solución Algebraica: Las Ecuaciones Normales\n",
    "\n",
    "La condición geométrica de que el error $\\vec{e}$ es ortogonal al espacio columna de $X$ nos lleva directamente a una solución algebraica elegante.\n",
    "\n",
    "#### Derivación Detallada\n",
    "1.  La condición de ortogonalidad significa que el producto punto de $\\vec{e}$ con **cada columna** de $X$ es cero. \n",
    "2.  Podemos escribir esto de forma compacta como: $ X^T \\vec{e} = \\vec{0} $.\n",
    "3.  Sustituimos la definición del error, $\\vec{e} = \\vec{y} - \\hat{y}$. Y como $\\hat{y} = X\\hat{\\beta}$, tenemos $\\vec{e} = \\vec{y} - X\\hat{\\beta}$.\n",
    "    $$ X^T (\\vec{y} - X\\hat{\\beta}) = \\vec{0} $$\n",
    "4.  Finalmente, distribuimos $X^T$ y reordenamos para obtener las **Ecuaciones Normales**:\n",
    "    $$ X^T X \\hat{\\beta} = X^T \\vec{y} $$\n",
    "\n",
    "¡Este es un sistema **cuadrado y simétrico** que podemos resolver para $\\hat{\\beta}$! La matriz $X^T X$ a veces se llama la \"matriz de Gram\". Si las columnas de $X$ son linealmente independientes, $X^T X$ es invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo Demostrativo 2: Resolviendo OLS con las Ecuaciones Normales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DATOS: Usamos nuestro Hilo Conductor.\n",
    "X_feature = datos_estudiantes[['horas_estudio']].values\n",
    "X = np.c_[np.ones(X_feature.shape[0]), X_feature]\n",
    "y = datos_estudiantes['calificacion_examen'].values\n",
    "\n",
    "# 2. CONSTRUIR LAS ECUACIONES NORMALES\n",
    "# Lado izquierdo de la ecuación\n",
    "XTX = X.T @ X\n",
    "# Lado derecho de la ecuación\n",
    "XTy = X.T @ y\n",
    "\n",
    "print(f\"Matriz XᵀX (2x2):\")\n",
    "print(np.round(XTX, 2))\n",
    "print(f\"\\nVector Xᵀy (2x1):\")\n",
    "print(np.round(XTy, 2))\n",
    "\n",
    "# 3. RESOLVER EL SISTEMA CUADRADO: (XᵀX)β = (Xᵀy)\n",
    "# Como XᵀX es cuadrada, ahora podemos usar np.linalg.solve()\n",
    "beta_hat = np.linalg.solve(XTX, XTy)\n",
    "\n",
    "# 4. INTERPRETACIÓN\n",
    "print(f\"\\nCoeficientes β_hat resueltos: {np.round(beta_hat, 2)}\")\n",
    "\n",
    "# Comparamos con el resultado de lstsq del notebook anterior\n",
    "beta_lstsq, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n",
    "print(f\"Coeficientes β_hat con lstsq: {np.round(beta_lstsq, 2)}\")\n",
    "print(f\"¿Son iguales? {np.allclose(beta_hat, beta_lstsq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Ejercicios Guiados con Scaffolding (8+)\n",
    "Rellena las partes marcadas con `# COMPLETAR` para afianzar tu comprensión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 1: Construir los Componentes de las Ecuaciones Normales ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS: Una matriz de diseño X y un vector y simples.\n",
    "X = np.array([[1, 2], [1, 3], [1, 5]])\n",
    "y = np.array([3, 4, 6])\n",
    "\n",
    "# TODO 1: Calcula la matriz de Gram, XᵀX.\n",
    "XTX = # COMPLETAR\n",
    "\n",
    "# TODO 2: Calcula el vector del lado derecho, Xᵀy.\n",
    "XTy = # COMPLETAR\n",
    "\n",
    "# VERIFICACIÓN\n",
    "XTX_esperada = np.array([[3, 10], [10, 38]])\n",
    "XTy_esperada = np.array([13, 47])\n",
    "assert np.allclose(XTX, XTX_esperada)\n",
    "assert np.allclose(XTy, XTy_esperada)\n",
    "print(\"✅ ¡Componentes de las Ecuaciones Normales calculados correctamente!\")\n",
    "print(f\"XᵀX =\\n{XTX}\")\n",
    "print(f\"\\nXᵀy = {XTy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 2: Resolver las Ecuaciones Normales ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS: Los componentes XᵀX y Xᵀy del ejercicio anterior.\n",
    "XTX = np.array([[3, 10], [10, 38]])\n",
    "XTy = np.array([13, 47])\n",
    "\n",
    "# TODO: Resuelve el sistema cuadrado (XᵀX)β = (Xᵀy) para encontrar beta_hat.\n",
    "# PISTA: Usa np.linalg.solve().\n",
    "beta_hat = # COMPLETAR\n",
    "\n",
    "# VERIFICACIÓN\n",
    "assert beta_hat.shape == (2,)\n",
    "assert np.allclose(beta_hat, np.array([1.5, 0.85714]))\n",
    "print(\"✅ ¡Sistema de Ecuaciones Normales resuelto correctamente!\")\n",
    "print(f\"β_hat = {np.round(beta_hat, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 3: Calcular el Vector de Proyección y el Error ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS: La matriz X original y el beta_hat que acabamos de encontrar.\n",
    "X = np.array([[1, 2], [1, 3], [1, 5]])\n",
    "y = np.array([3, 4, 6])\n",
    "beta_hat = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "\n",
    "# TODO 1: Calcula el vector de predicciones y_hat (la proyección de y sobre Col(X)).\n",
    "y_hat = # COMPLETAR\n",
    "\n",
    "# TODO 2: Calcula el vector de error (residuos).\n",
    "error = # COMPLETAR\n",
    "\n",
    "# VERIFICACIÓN\n",
    "assert y_hat.shape == (3,)\n",
    "assert error.shape == (3,)\n",
    "print(\"✅ ¡Cálculos correctos!\")\n",
    "print(f\"y_hat (proyección) = {np.round(y_hat, 2)}\")\n",
    "print(f\"error (residuos) = {np.round(error, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 4: Verificar la Ortogonalidad del Error ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS: La X, y_hat y error del ejercicio anterior.\n",
    "X = np.array([[1, 2], [1, 3], [1, 5]])\n",
    "y = np.array([3, 4, 6])\n",
    "y_hat = X @ np.linalg.solve(X.T @ X, X.T @ y)\n",
    "error = y - y_hat\n",
    "\n",
    "# El vector de error debe ser ortogonal a CADA columna de X.\n",
    "\n",
    "# TODO 1: Calcula el producto punto del error con la primera columna de X.\n",
    "dot_col1 = # COMPLETAR\n",
    "\n",
    "# TODO 2: Calcula el producto punto del error con la segunda columna de X.\n",
    "dot_col2 = # COMPLETAR\n",
    "\n",
    "# VERIFICACIÓN\n",
    "assert np.isclose(dot_col1, 0) and np.isclose(dot_col2, 0)\n",
    "# Una forma más compacta de verificar es X.T @ error, que debe ser un vector de ceros.\n",
    "assert np.allclose(X.T @ error, np.zeros(2))\n",
    "print(\"✅ ¡Verificación de ortogonalidad exitosa!\")\n",
    "print(\"El vector de error es perpendicular al espacio de las predicciones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# 5. Banco de Ejercicios Prácticos (30+)\n",
    "Ahora te toca a ti. Resuelve estos ejercicios para consolidar tu conocimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte A: Componentes de las Ecuaciones Normales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A1 (🟢 Fácil):** Dados $A = \\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 3 \\end{pmatrix}$ y $\\vec{b} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix}$, calcula a mano (y luego verifica con código) $A^T A$ y $A^T \\vec{b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A2 (🟢 Fácil):** Usando el `datos_estudiantes`, construye la matriz de diseño $X$ y el vector $y$. Calcula $X^T X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A3 (🟡 Medio):** ¿Por qué la matriz $X^T X$ es siempre cuadrada, incluso si $X$ es una matriz alta y delgada?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A4 (🟡 Medio):** Usando `datos_negocio`, construye la matriz de diseño $X$ para predecir `ventas_mensuales` a partir de `precio` y `gasto_marketing`. Calcula $X^T X$. ¿De qué tamaño es?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte B: Resolución y Verificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B1 (🟢 Fácil):** Usando los resultados de A1, resuelve las Ecuaciones Normales para encontrar $\\hat{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B2 (🟡 Medio):** Para los datos de A1, calcula el vector de proyección $\\vec{p}=A\\hat{x}$ y el vector de error $\\vec{e}=\\vec{b}-\\vec{p}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B3 (🟡 Medio):** Verifica que el vector de error $\\vec{e}$ del ejercicio B2 es ortogonal a las columnas de A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B4 (🔴 Reto):** Implementa desde cero la regresión múltiple para el problema de `datos_negocio` (ejercicio A4). Resuelve las Ecuaciones Normales para encontrar los 3 coeficientes beta. Compara tu resultado con `np.linalg.lstsq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte C: Geometría y Aplicaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C1 (🟡 Medio):** ¿Qué le pasaría a la matriz $X^T X$ si las columnas de $X$ fueran perfectamente ortogonales? ¿Cómo simplificaría esto la solución de las Ecuaciones Normales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C2 (🔴 Reto):** La matriz de Proyección que mapea cualquier vector $\\vec{y}$ a su proyección sobre el espacio columna de $X$ es $P = X(X^T X)^{-1} X^T$. Usando los datos de A1, calcula esta matriz $P$. Verifica que $P\\vec{b}$ te da el mismo vector de proyección $\\vec{p}$ que calculaste en B2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C3 (🔴 Reto):** Una propiedad de las matrices de proyección es que son idempotentes ($P^2 = P$). Verifica que la matriz $P$ que calculaste en C2 cumple esta propiedad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Mini-Quiz de Autoevaluación\n",
    "\n",
    "1. ¿Cuál es la fórmula de las Ecuaciones Normales?\n",
    "2. ¿Cuál es la relación geométrica entre el vector de error $\\vec{e}$ y el espacio columna de la matriz $X$ en una regresión OLS?\n",
    "3. Si las columnas de $X$ son linealmente dependientes (multicolinealidad), ¿qué problema ocurre con la matriz $X^T X$ que impide resolver las Ecuaciones Normales?\n",
    "4. La solución de mínimos cuadrados, $\\hat{\\beta}$, produce un vector de predicciones $\\hat{y} = X\\hat{\\beta}$. Este vector $\\hat{y}$ es la ___________ de $\\vec{y}$ sobre el espacio columna de $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Próximos Pasos\n",
    "\n",
    "¡Felicidades! Has entendido la bella geometría que hace funcionar la regresión lineal, derivando y aplicando su solución desde los primeros principios.\n",
    "\n",
    "- Aunque las Ecuaciones Normales son conceptualmente clave, pueden ser numéricamente inestables si $X^T X$ está mal condicionada (alto número de condición). En el **próximo notebook** exploraremos métodos numéricamente más robustos para resolver problemas de mínimos cuadrados, como la **descomposición QR**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 10.x",
   "language": "sagemath",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
