{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.4.2: Proyecciones y M√≠nimos Cuadrados Ordinarios (OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos de Aprendizaje\n",
    "\n",
    "Al completar este notebook, ser√°s capaz de:\n",
    "\n",
    "- **Interpretar** geom√©tricamente la soluci√≥n de m√≠nimos cuadrados como la **proyecci√≥n ortogonal** de un vector sobre un subespacio.\n",
    "- **Derivar y entender** las **Ecuaciones Normales**: $X^T X \\hat{\\beta} = X^T \\vec{y}$, la base del OLS.\n",
    "- **Verificar** la condici√≥n fundamental de ortogonalidad del vector de error ($X^T \\vec{e} = \\vec{0}$).\n",
    "- **Construir** y resolver un modelo de regresi√≥n lineal 'desde cero' usando este m√©todo, en lugar de funciones de alto nivel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Celda de Configuraci√≥n (Oculta) ---\n",
    "%display latex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## ‚öôÔ∏è El Arsenal de Datasets: Nuestra Fuente de Ejercicios\n",
    "\n",
    "Para este notebook, que se sumerge en la mec√°nica interna de OLS, usaremos datasets que nos permitan visualizar claramente la geometr√≠a de las proyecciones y verificar las propiedades matem√°ticas de la soluci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURACI√ìN DE DATASETS ===\n",
    "from src.data_generation.create_student_performance import create_student_performance_data\n",
    "from src.data_generation.create_business_data import create_business_data\n",
    "from src.data_generation.create_geometric_shapes import create_geometric_shapes\n",
    "\n",
    "# Configuraci√≥n centralizada de aleatoriedad para REPRODUCIBILIDAD\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "# === Generaci√≥n de Datasets y Matrices para este Notebook ===\n",
    "\n",
    "# üí° CONTEXTO PEDAG√ìGICO: Hilo Conductor (Regresi√≥n OLS)\n",
    "# Usaremos el dataset de estudiantes para implementar la soluci√≥n de OLS a trav√©s de las\n",
    "# Ecuaciones Normales, verificando que obtenemos el mismo resultado que con `lstsq`.\n",
    "datos_estudiantes = create_student_performance_data(rng, simplified=True, n_samples=100)\n",
    "\n",
    "# üí° CONTEXTO PEDAG√ìGICO: Geometr√≠a de la Proyecci√≥n\n",
    "# Para visualizar la idea de proyectar un vector sobre un plano, crearemos un ejemplo\n",
    "# simple y de baja dimensi√≥n donde podamos graficar todos los elementos: el espacio, el vector y su proyecci√≥n.\n",
    "X_plano = create_geometric_shapes(rng, 'line', n_samples=10).values # Generamos una base para un plano\n",
    "X_plano = np.c_[X_plano, X_plano[:, 0] - X_plano[:, 1]] # Creamos un plano en R^3\n",
    "\n",
    "# üí° CONTEXTO PEDAG√ìGICO: Regresi√≥n M√∫ltiple con Ecuaciones Normales\n",
    "# Demostraremos que la belleza de las Ecuaciones Normales es que funcionan igual de bien\n",
    "# para la regresi√≥n m√∫ltiple, usando el dataset de negocio.\n",
    "datos_negocio = create_business_data(rng, n_samples=150)\n",
    "\n",
    "print(\"Datasets generados y listos para usar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. La Geometr√≠a de la \"Mejor Soluci√≥n\"\n",
    "\n",
    "En el notebook anterior, aceptamos que `np.linalg.lstsq` nos daba la \"mejor soluci√≥n\" a un sistema inconsistente $X\\vec{\\beta} = \\vec{y}$. Pero, ¬øqu√© significa \"mejor\" geom√©tricamente?\n",
    "\n",
    "1.  **El Problema:** El vector de nuestros datos reales, $\\vec{y}$ (las calificaciones), no se encuentra en el subespacio generado por las columnas de $X$, que llamamos **Espacio Columna** de $X$, o $\\text{Col}(X)$. $\\text{Col}(X)$ representa el universo de todas las posibles predicciones lineales que podemos hacer.\n",
    "\n",
    "2.  **La Soluci√≥n:** Si no podemos alcanzar $\\vec{y}$ exactamente, la \"mejor soluci√≥n\" es el vector $\\hat{y}$ (o $\\vec{p}$) *dentro* de $\\text{Col}(X)$ que est√° **m√°s cerca** de $\\vec{y}$. Este punto es la **proyecci√≥n ortogonal** de $\\vec{y}$ sobre el subespacio $\\text{Col}(X)$.\n",
    "\n",
    "**La Gran Intuici√≥n:** La distancia m√°s corta de un punto (la punta de $\\vec{y}$) a un plano ($\\\text{Col}(X)$) se logra con una l√≠nea perpendicular. Esto significa que el **vector de error**, $\\vec{e} = \\vec{y} - \\hat{y}$, debe ser **ortogonal** a *todo* el espacio columna de $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo Demostrativo 1: Visualizando la Proyecci√≥n Ortogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DATOS: Creamos un \"plano\" (espacio columna de X) y un vector \"y\" que no est√° en el plano.\n",
    "X = np.array([[1, 0, 1], [0, 1, 1], [1, 1, 2], [1, -1, 0]]) # X es 4x3 pero tiene rango 2\n",
    "y = np.array([4, 5, 10, 2])\n",
    "\n",
    "# 2. APLICACI√ìN: Encontramos la proyecci√≥n de y sobre el espacio columna de X usando lstsq.\n",
    "beta_hat, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n",
    "y_hat = X @ beta_hat # Esta es la proyecci√≥n 'p'\n",
    "error_vec = y - y_hat\n",
    "\n",
    "# 3. INTERPRETACI√ìN Y VERIFICACI√ìN\n",
    "print(f\"Vector y (original): {y}\")\n",
    "print(f\"Vector ≈∑ (proyecci√≥n): {np.round(y_hat, 2)}\")\n",
    "print(f\"Vector de error (e): {np.round(error_vec, 2)}\")\n",
    "\n",
    "# Verificamos que el error es ortogonal a las columnas de X\n",
    "orto_check = X.T @ error_vec\n",
    "print(f\"\\nVerificaci√≥n de Ortogonalidad (X.T @ e): {np.round(orto_check, 9)}\")\n",
    "print(\"Los valores son pr√°cticamente cero, confirmando la ortogonalidad.\")\n",
    "\n",
    "# (La visualizaci√≥n 3D/4D es compleja, pero el principio num√©rico se mantiene)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. La Soluci√≥n Algebraica: Las Ecuaciones Normales\n",
    "\n",
    "La condici√≥n geom√©trica de que el error $\\vec{e}$ es ortogonal al espacio columna de $X$ nos lleva directamente a una soluci√≥n algebraica elegante.\n",
    "\n",
    "#### Derivaci√≥n Detallada\n",
    "1.  La condici√≥n de ortogonalidad significa que el producto punto de $\\vec{e}$ con **cada columna** de $X$ es cero. \n",
    "2.  Podemos escribir esto de forma compacta como: $ X^T \\vec{e} = \\vec{0} $.\n",
    "3.  Sustituimos la definici√≥n del error, $\\vec{e} = \\vec{y} - \\hat{y}$. Y como $\\hat{y} = X\\hat{\\beta}$, tenemos $\\vec{e} = \\vec{y} - X\\hat{\\beta}$.\n",
    "    $$ X^T (\\vec{y} - X\\hat{\\beta}) = \\vec{0} $$\n",
    "4.  Finalmente, distribuimos $X^T$ y reordenamos para obtener las **Ecuaciones Normales**:\n",
    "    $$ X^T X \\hat{\\beta} = X^T \\vec{y} $$\n",
    "\n",
    "¬°Este es un sistema **cuadrado y sim√©trico** que podemos resolver para $\\hat{\\beta}$! La matriz $X^T X$ a veces se llama la \"matriz de Gram\". Si las columnas de $X$ son linealmente independientes, $X^T X$ es invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo Demostrativo 2: Resolviendo OLS con las Ecuaciones Normales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DATOS: Usamos nuestro Hilo Conductor.\n",
    "X_feature = datos_estudiantes[['horas_estudio']].values\n",
    "X = np.c_[np.ones(X_feature.shape[0]), X_feature]\n",
    "y = datos_estudiantes['calificacion_examen'].values\n",
    "\n",
    "# 2. CONSTRUIR LAS ECUACIONES NORMALES\n",
    "# Lado izquierdo de la ecuaci√≥n\n",
    "XTX = X.T @ X\n",
    "# Lado derecho de la ecuaci√≥n\n",
    "XTy = X.T @ y\n",
    "\n",
    "print(f\"Matriz X·µÄX (2x2):\")\n",
    "print(np.round(XTX, 2))\n",
    "print(f\"\\nVector X·µÄy (2x1):\")\n",
    "print(np.round(XTy, 2))\n",
    "\n",
    "# 3. RESOLVER EL SISTEMA CUADRADO: (X·µÄX)Œ≤ = (X·µÄy)\n",
    "# Como X·µÄX es cuadrada, ahora podemos usar np.linalg.solve()\n",
    "beta_hat = np.linalg.solve(XTX, XTy)\n",
    "\n",
    "# 4. INTERPRETACI√ìN\n",
    "print(f\"\\nCoeficientes Œ≤_hat resueltos: {np.round(beta_hat, 2)}\")\n",
    "\n",
    "# Comparamos con el resultado de lstsq del notebook anterior\n",
    "beta_lstsq, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n",
    "print(f\"Coeficientes Œ≤_hat con lstsq: {np.round(beta_lstsq, 2)}\")\n",
    "print(f\"¬øSon iguales? {np.allclose(beta_hat, beta_lstsq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Ejercicios Guiados con Scaffolding (8+)\n",
    "Rellena las partes marcadas con `# COMPLETAR` para afianzar tu comprensi√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 1: Construir los Componentes de las Ecuaciones Normales ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS: Una matriz de dise√±o X y un vector y simples.\n",
    "X = np.array([[1, 2], [1, 3], [1, 5]])\n",
    "y = np.array([3, 4, 6])\n",
    "\n",
    "# TODO 1: Calcula la matriz de Gram, X·µÄX.\n",
    "XTX = # COMPLETAR\n",
    "\n",
    "# TODO 2: Calcula el vector del lado derecho, X·µÄy.\n",
    "XTy = # COMPLETAR\n",
    "\n",
    "# VERIFICACI√ìN\n",
    "XTX_esperada = np.array([[3, 10], [10, 38]])\n",
    "XTy_esperada = np.array([13, 47])\n",
    "assert np.allclose(XTX, XTX_esperada)\n",
    "assert np.allclose(XTy, XTy_esperada)\n",
    "print(\"‚úÖ ¬°Componentes de las Ecuaciones Normales calculados correctamente!\")\n",
    "print(f\"X·µÄX =\\n{XTX}\")\n",
    "print(f\"\\nX·µÄy = {XTy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 2: Resolver las Ecuaciones Normales ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS: Los componentes X·µÄX y X·µÄy del ejercicio anterior.\n",
    "XTX = np.array([[3, 10], [10, 38]])\n",
    "XTy = np.array([13, 47])\n",
    "\n",
    "# TODO: Resuelve el sistema cuadrado (X·µÄX)Œ≤ = (X·µÄy) para encontrar beta_hat.\n",
    "# PISTA: Usa np.linalg.solve().\n",
    "beta_hat = # COMPLETAR\n",
    "\n",
    "# VERIFICACI√ìN\n",
    "assert beta_hat.shape == (2,)\n",
    "assert np.allclose(beta_hat, np.array([1.5, 0.85714]))\n",
    "print(\"‚úÖ ¬°Sistema de Ecuaciones Normales resuelto correctamente!\")\n",
    "print(f\"Œ≤_hat = {np.round(beta_hat, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 3: Calcular el Vector de Proyecci√≥n y el Error ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS: La matriz X original y el beta_hat que acabamos de encontrar.\n",
    "X = np.array([[1, 2], [1, 3], [1, 5]])\n",
    "y = np.array([3, 4, 6])\n",
    "beta_hat = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "\n",
    "# TODO 1: Calcula el vector de predicciones y_hat (la proyecci√≥n de y sobre Col(X)).\n",
    "y_hat = # COMPLETAR\n",
    "\n",
    "# TODO 2: Calcula el vector de error (residuos).\n",
    "error = # COMPLETAR\n",
    "\n",
    "# VERIFICACI√ìN\n",
    "assert y_hat.shape == (3,)\n",
    "assert error.shape == (3,)\n",
    "print(\"‚úÖ ¬°C√°lculos correctos!\")\n",
    "print(f\"y_hat (proyecci√≥n) = {np.round(y_hat, 2)}\")\n",
    "print(f\"error (residuos) = {np.round(error, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === EJERCICIO GUIADO 4: Verificar la Ortogonalidad del Error ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS: La X, y_hat y error del ejercicio anterior.\n",
    "X = np.array([[1, 2], [1, 3], [1, 5]])\n",
    "y = np.array([3, 4, 6])\n",
    "y_hat = X @ np.linalg.solve(X.T @ X, X.T @ y)\n",
    "error = y - y_hat\n",
    "\n",
    "# El vector de error debe ser ortogonal a CADA columna de X.\n",
    "\n",
    "# TODO 1: Calcula el producto punto del error con la primera columna de X.\n",
    "dot_col1 = # COMPLETAR\n",
    "\n",
    "# TODO 2: Calcula el producto punto del error con la segunda columna de X.\n",
    "dot_col2 = # COMPLETAR\n",
    "\n",
    "# VERIFICACI√ìN\n",
    "assert np.isclose(dot_col1, 0) and np.isclose(dot_col2, 0)\n",
    "# Una forma m√°s compacta de verificar es X.T @ error, que debe ser un vector de ceros.\n",
    "assert np.allclose(X.T @ error, np.zeros(2))\n",
    "print(\"‚úÖ ¬°Verificaci√≥n de ortogonalidad exitosa!\")\n",
    "print(\"El vector de error es perpendicular al espacio de las predicciones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# 5. Banco de Ejercicios Pr√°cticos (30+)\n",
    "Ahora te toca a ti. Resuelve estos ejercicios para consolidar tu conocimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte A: Componentes de las Ecuaciones Normales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A1 (üü¢ F√°cil):** Dados $A = \\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 3 \\end{pmatrix}$ y $\\vec{b} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix}$, calcula a mano (y luego verifica con c√≥digo) $A^T A$ y $A^T \\vec{b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A2 (üü¢ F√°cil):** Usando el `datos_estudiantes`, construye la matriz de dise√±o $X$ y el vector $y$. Calcula $X^T X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A3 (üü° Medio):** ¬øPor qu√© la matriz $X^T X$ es siempre cuadrada, incluso si $X$ es una matriz alta y delgada?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A4 (üü° Medio):** Usando `datos_negocio`, construye la matriz de dise√±o $X$ para predecir `ventas_mensuales` a partir de `precio` y `gasto_marketing`. Calcula $X^T X$. ¬øDe qu√© tama√±o es?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte B: Resoluci√≥n y Verificaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B1 (üü¢ F√°cil):** Usando los resultados de A1, resuelve las Ecuaciones Normales para encontrar $\\hat{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B2 (üü° Medio):** Para los datos de A1, calcula el vector de proyecci√≥n $\\vec{p}=A\\hat{x}$ y el vector de error $\\vec{e}=\\vec{b}-\\vec{p}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B3 (üü° Medio):** Verifica que el vector de error $\\vec{e}$ del ejercicio B2 es ortogonal a las columnas de A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B4 (üî¥ Reto):** Implementa desde cero la regresi√≥n m√∫ltiple para el problema de `datos_negocio` (ejercicio A4). Resuelve las Ecuaciones Normales para encontrar los 3 coeficientes beta. Compara tu resultado con `np.linalg.lstsq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte C: Geometr√≠a y Aplicaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C1 (üü° Medio):** ¬øQu√© le pasar√≠a a la matriz $X^T X$ si las columnas de $X$ fueran perfectamente ortogonales? ¬øC√≥mo simplificar√≠a esto la soluci√≥n de las Ecuaciones Normales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C2 (üî¥ Reto):** La matriz de Proyecci√≥n que mapea cualquier vector $\\vec{y}$ a su proyecci√≥n sobre el espacio columna de $X$ es $P = X(X^T X)^{-1} X^T$. Usando los datos de A1, calcula esta matriz $P$. Verifica que $P\\vec{b}$ te da el mismo vector de proyecci√≥n $\\vec{p}$ que calculaste en B2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C3 (üî¥ Reto):** Una propiedad de las matrices de proyecci√≥n es que son idempotentes ($P^2 = P$). Verifica que la matriz $P$ que calculaste en C2 cumple esta propiedad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Mini-Quiz de Autoevaluaci√≥n\n",
    "\n",
    "1. ¬øCu√°l es la f√≥rmula de las Ecuaciones Normales?\n",
    "2. ¬øCu√°l es la relaci√≥n geom√©trica entre el vector de error $\\vec{e}$ y el espacio columna de la matriz $X$ en una regresi√≥n OLS?\n",
    "3. Si las columnas de $X$ son linealmente dependientes (multicolinealidad), ¬øqu√© problema ocurre con la matriz $X^T X$ que impide resolver las Ecuaciones Normales?\n",
    "4. La soluci√≥n de m√≠nimos cuadrados, $\\hat{\\beta}$, produce un vector de predicciones $\\hat{y} = X\\hat{\\beta}$. Este vector $\\hat{y}$ es la ___________ de $\\vec{y}$ sobre el espacio columna de $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Pr√≥ximos Pasos\n",
    "\n",
    "¬°Felicidades! Has entendido la bella geometr√≠a que hace funcionar la regresi√≥n lineal, derivando y aplicando su soluci√≥n desde los primeros principios.\n",
    "\n",
    "- Aunque las Ecuaciones Normales son conceptualmente clave, pueden ser num√©ricamente inestables si $X^T X$ est√° mal condicionada (alto n√∫mero de condici√≥n). En el **pr√≥ximo notebook** exploraremos m√©todos num√©ricamente m√°s robustos para resolver problemas de m√≠nimos cuadrados, como la **descomposici√≥n QR**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 10.x",
   "language": "sagemath",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
